<html>
<head>
<title>AI desperately needs regulation and public accountability, experts say • TechCrunch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">专家称，人工智能迫切需要监管和公共问责</h1>
<blockquote>原文：<a href="https://web.archive.org/web/https://techcrunch.com/2018/12/07/ai-desperately-needs-regulation-and-public-accountability-experts-say/">https://web.archive.org/web/https://techcrunch.com/2018/12/07/ai-desperately-needs-regulation-and-public-accountability-experts-say/</a></blockquote><div><div class="article-content">
				<p id="speakable-summary" class="translated">根据谷歌、微软和AI Now其他公司的研究人员的一份新报告，人工智能系统和创造者迫切需要政府和人权监督机构的直接干预。令人惊讶的是，科技行业似乎并不擅长自我监管。</p>
<p class="translated">在本周发布的40页报告(PDF)中，这家总部位于纽约大学的组织(与微软研究院和谷歌有关联的成员)显示，基于人工智能的工具在部署时很少考虑潜在的负面影响，甚至没有记录好的影响。虽然这是一件事，如果它发生在这里或那里的受控试验中，相反，这些未经测试、未记录的人工智能系统正在投入使用，它们可能会深深影响成千上万的人。</p>
<p class="translated">我不会在这里举例子，但想想边境巡逻，整个学区和警察部门，等等。这些系统正在造成真正的伤害，不仅没有系统来阻止它们，甚至很少有人跟踪和量化这种伤害。</p>
<p class="translated">“目前管理人工智能的框架无法确保问责制，”研究人员在论文中写道。“随着这些系统的普遍性、复杂性和规模的增长，缺乏有意义的问责和监督——包括责任、义务和正当程序的基本保障——成为日益紧迫的问题。”</p>
<p class="translated">目前，公司正在创造基于人工智能的解决方案，从给学生评分到评估移民的犯罪行为。创建这些项目的公司只受他们自己决定的一些道德声明的约束。</p>
<p class="translated">例如，谷歌最近<a href="https://web.archive.org/web/20221221041139/https://techcrunch.com/2018/06/07/googles-new-ai-principles-forbid-its-use-in-weapons-and-human-rights-violations/">在为国防部工作引起轩然大波后，大谈设定一些“人工智能原则”</a>。该公司表示，其人工智能工具将对社会有益，负责任，不会违反广泛接受的人权原则。</p>
<p class="translated">很自然，事实证明该公司一直在<a href="https://web.archive.org/web/20221221041139/https://techcrunch.com/2018/08/01/google-china-return-search/">为中国</a>开发一个原型审查搜索引擎。干得好！</p>

<p class="translated">因此，现在我们确切地知道在多大程度上可以信任那家公司来设定自己的界限。我们不妨假设脸书就是这种情况，它正在使用基于人工智能的工具进行调节；亚马逊，它公开追求以监控为目的的人工智能；还有微软，昨天<a href="https://web.archive.org/web/20221221041139/https://techcrunch.com/2018/12/06/microsoft-calls-on-companies-to-adopt-a-facial-recognition-code-of-conduct/">发表了一篇关于人工智能伦理的好文章</a>——但是尽管它的意图看起来很好，一个<a href="https://web.archive.org/web/20221221041139/https://techcrunch.com/2018/03/14/a-hippocratic-oath-for-artificial-intelligence-practitioners/">“道德准则”</a>只不过是一个公司可以随时违反的承诺。</p>
<p class="translated">AI Now报告有许多建议，我在下面总结了这些建议，但它们确实值得一读。它可读性很强，是一篇很好的评论，分析也很聪明。</p>
<ul>
<li class="translated">监管是迫切需要的。但是一个“国家人工智能安全机构”或类似的东西是不切实际的。相反，卫生或交通运输等行业的人工智能专家应该考虑更新特定领域的规则，以包括限制和定义机器学习工具角色的条款。我们不需要一个人工智能部门，但联邦航空局应该准备好评估，比如说，机器学习辅助的空中交通管制系统的合法性。</li>
<li class="translated">面部识别，特别是它的可疑应用，如情绪和犯罪检测，需要仔细检查，并受到虚假广告和欺诈性药物的限制。</li>
<li class="translated">从数据集到决策过程，公共问责和记录需要成为规则，包括系统的内部操作。这些不仅对于使用给定系统的基本审计和证明是必要的，而且对于法律目的来说，如果这样的决定被系统分类或影响的人质疑，也是必要的。公司需要放下自尊，记录这些事情，即使他们宁愿把它们作为商业秘密——在我看来，这是报告中最大的问题。</li>
<li class="translated">AI问责过程中需要建立更多的经费和更多的先例；对于美国公民自由联盟来说，写一篇关于市政“自动决策系统”的文章是不够的，这种系统剥夺了某些阶层的人的权利。这些事情需要诉诸法庭，受影响的人需要反馈机制。</li>
<li class="translated">整个人工智能行业需要摆脱其工程和计算机科学的摇篮——新工具和能力跨越边界和学科，应该在研究中考虑，而不仅仅是技术方面。“扩大人工智能研究的学科方向将确保更深入地关注社会背景，并在这些系统应用于人类时更加关注潜在的危险，”研究人员写道。</li>
</ul>
<p class="translated">这些都是很好的建议，但不是那种可以在短时间内做出的建议，所以预计2019年将是又一个失误和失实陈述的泥沼。和往常一样，永远不要相信一家公司说的话，只相信它做的事——即使这样，也不要相信它说它做的事。</p>
			</div>

			</div>    
</body>
</html>