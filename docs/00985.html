<html>
<head>
<title>Amnesty International used machine-learning to quantify the scale of abuse against women on Twitter • TechCrunch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">大赦国际使用机器学习来量化 Twitter TechCrunch 上针对女性的虐待程度</h1>
<blockquote>原文：<a href="https://web.archive.org/web/https://techcrunch.com/2018/12/18/amnesty-international-used-machine-learning-to-quantify-the-scale-of-abuse-against-women-on-twitter/">https://web.archive.org/web/https://techcrunch.com/2018/12/18/amnesty-international-used-machine-learning-to-quantify-the-scale-of-abuse-against-women-on-twitter/</a></blockquote><div><div class="article-content">
				<p id="speakable-summary" class="translated"><em>更新:Twitter 的回应已被添加到本文的末尾。</em></p>
<p class="translated">大赦国际和 Element AI 的一项新研究试图用数字来说明许多女性已经知道的一个问题:Twitter 是一个骚扰和虐待的污水池。这项研究在 6500 名志愿者的帮助下进行，被大赦国际称为“有史以来最大的”针对女性在线虐待的研究，使用 Element AI 的机器学习软件分析了 2017 年发送给 778 名女性政治家和记者的推文。它发现，这些推文中有 7.1%，即 110 万条，要么是“有问题的”，要么是“辱骂的”，大赦国际表示，这相当于每 30 秒发送一条辱骂推文。</p>
<p class="translated">在一个<a href="https://web.archive.org/web/20221208124750/https://decoders.amnesty.org/projects/troll-patrol/findings">互动网站上，该网站公布了这项研究的方法和结果</a>，人权倡导组织表示，许多女性要么审查她们发布的内容，限制她们在 Twitter 上的互动，要么干脆完全退出这个平台:“在这个分水岭时刻，全世界的女性都在利用她们的集体力量，通过社交媒体平台放大她们的声音，Twitter 未能一致、透明地执行自己的社区标准来解决暴力和虐待问题，这意味着女性正在被推向沉默文化。”</p>
<p class="translated">大赦国际在过去的两年里一直在研究 Twitter 上对女性的虐待，在今年早些时候发布了一份报告后，该组织注册了 6500 名志愿者，他们称之为“巨魔巡逻队”，该报告称<a href="https://web.archive.org/web/20221208124750/https://www.amnesty.org/en/latest/research/2018/03/online-violence-against-women-chapter-1/">将 Twitter 描述为女性的“有毒”场所</a>。</p>
<p class="translated">总的来说，志愿者们分析了 2017 年 1 月至 12 月期间向 778 名女性发送的 288，000 条推文，这些女性包括英国和美国政界的政治家和记者。政治家包括英国议会和美国国会的成员，而记者代表各种出版物，包括《每日邮报》、《纽约时报》、《卫报》、《太阳报》、《gal-dem》、《粉红新闻》和《布莱巴特》。</p>
<p class="translated">巨魔巡逻队的志愿者来自 150 个国家，年龄从 18 岁到 70 岁不等，他们接受了关于什么是有问题或辱骂性推文的培训。然后给他们看提到 778 名女性之一的匿名推文，并问他们这些推文是否有问题或辱骂。每条推文都给几名志愿者看。此外，大赦国际表示，“三名暴力和虐待妇女问题专家”还对 1000 条推文进行了分类，以“确保我们能够评估我们的数字志愿者标记的推文的质量。”</p>
<p class="translated">该研究将“有问题的”定义为“包含伤害或敌意内容的推文，特别是如果在多个场合重复给一个人，但不一定达到滥用的门槛，”而“滥用”则意味着推文“违反 Twitter 自己的规则，包括基于种族、族裔、民族血统、性取向、性别、性别认同、宗教信仰、年龄、残疾或严重疾病而促进暴力或威胁他人的内容。”</p>
<p class="translated"><img decoding="async" loading="lazy" class="aligncenter size-large wp-image-1760636" src="../Images/bde67956508460597b5a4f0ead67503a.png" alt="" srcset="https://web.archive.org/web/20221208124750im_/https://techcrunch.com/wp-content/uploads/2018/12/Screen-Shot-2018-12-19-at-12.50.14-PM.png 1006w, https://web.archive.org/web/20221208124750im_/https://techcrunch.com/wp-content/uploads/2018/12/Screen-Shot-2018-12-19-at-12.50.14-PM.png?resize=150,79 150w, https://web.archive.org/web/20221208124750im_/https://techcrunch.com/wp-content/uploads/2018/12/Screen-Shot-2018-12-19-at-12.50.14-PM.png?resize=300,158 300w, https://web.archive.org/web/20221208124750im_/https://techcrunch.com/wp-content/uploads/2018/12/Screen-Shot-2018-12-19-at-12.50.14-PM.png?resize=768,405 768w, https://web.archive.org/web/20221208124750im_/https://techcrunch.com/wp-content/uploads/2018/12/Screen-Shot-2018-12-19-at-12.50.14-PM.png?resize=680,358 680w, https://web.archive.org/web/20221208124750im_/https://techcrunch.com/wp-content/uploads/2018/12/Screen-Shot-2018-12-19-at-12.50.14-PM.png?resize=50,26 50w" sizes="(max-width: 680px) 100vw, 680px" data-original-src="https://web.archive.org/web/20221208124750im_/https://techcrunch.com/wp-content/uploads/2018/12/Screen-Shot-2018-12-19-at-12.50.14-PM.png?w=680"/></p>
<p class="translated">然后，使用 Element AI 的机器学习软件处理带标签的推文的子集，以推断 2017 年期间提到 778 名女性的总共 1450 万条推文的分析结果。(因为这项研究直到 2018 年 3 月才收集推文，大赦国际指出，滥用的规模可能更大，因为一些滥用的推文可能已经被删除或由被暂停或禁用的账户发出。)Element AI 的推断得出的结果是，发送给这些女性的推文有 7.1%是有问题的或辱骂性的，2017 年总计 110 万条推文。</p>
<p class="translated">黑人、亚裔、拉丁裔和混血女性在有问题或辱骂性的推文中被提及的可能性比白人女性高 34%。黑人女性尤其脆弱:在有问题或辱骂性的推文中，她们比白人女性被提及的可能性高 84%。在研究样本中，有十分之一的推文提到黑人女性有问题或辱骂，相比之下，白人女性只有十五分之一。</p>
<p class="translated">“我们发现，尽管虐待针对的是政治领域的女性，但有色人种女性更容易受到影响，黑人女性受到的伤害更大。Twitter 未能解决这个问题意味着它助长了已经被边缘化的声音的沉默，”大赦国际战术研究高级顾问米莲娜·马林在声明中说。</p>
<p class="translated">将结果按职业分类，研究发现，在提到研究中的 454 名记者的推文中，有 7%是有问题的或辱骂的。接受调查的 324 名政界人士的目标比率相似，7.12%的推文提到他们有问题或辱骂。</p>
<p class="translated">当然，从英美 778 名记者和政治家的样本中得出的结论很难推广到其他职业、国家或普通大众。然而，这项研究的发现很重要，因为许多政治家和记者需要使用社交媒体来有效地完成他们的工作。女性，尤其是有色人种女性，在这两个职业中的代表性都不足，许多人留在 Twitter 上只是为了发表一份关于可见性的声明，即使这意味着要应对不断的骚扰和虐待。此外，Twitter 的 API 变化意味着许多第三方反欺凌工具不再有效，正如科技记者 Sarah Jeong 在自己的 Twitter 个人资料中指出的那样，该平台尚未推出复制其功能的工具。</p>

<p/>

<p/>
<p class="translated">大赦国际关于推特上对女性的虐待行为的其他研究包括 2017 年对 8 个国家的女性进行的在线民意调查，以及<a href="https://web.archive.org/web/20221208124750/https://www.newstatesman.com/2017/09/we-tracked-25688-abusive-tweets-sent-women-mps-half-were-directed-diane-abbott">对英国 2017 年大选前女性议员面临的虐待的分析</a>。该组织表示，巨魔巡逻队不是为了“监管 Twitter 或迫使它删除内容。”相反，该组织希望该平台更加透明，特别是关于它用来检测滥用的机器学习算法。</p>
<p class="translated">由于最大的社交媒体平台现在依赖机器学习来扩展他们的反虐待监测，Element AI 还利用这项研究的数据开发了一个自动检测虐待性推文的机器学习模型。在接下来的三周里，该模型将在大赦国际的网站上进行测试，以“展示人工智能技术的潜力和目前的局限性。”这些限制意味着社交媒体平台需要非常仔细地微调它们的算法，以便在不标记合法言论的情况下检测滥用内容。</p>
<p class="translated">“这些权衡是基于价值的判断，对网上的言论自由和其他人权有严重影响，”该组织表示，并补充说，“就目前情况而言，自动化可能在评估趋势或标记内容以供人工审查方面发挥有用的作用，但它充其量应该用于协助受过训练的版主，当然不应该取代他们。”</p>
<p class="translated"><s> TechCrunch 已经联系了 Twitter 进行评论。</s> Twitter 引用了 12 月 12 日发给大赦国际的正式回复中的几段话，Vijaya Gadde 是 Twitter 的法律、政策、信任和安全全球负责人。</p>
<blockquote><p class="translated">“Twitter 已经公开承诺改善我们服务的公共对话的集体健康、开放性和文明程度。Twitter 的健康是通过我们如何帮助鼓励更健康的辩论、对话和批判性思维来衡量的。相反，滥用、恶意自动化和操纵有损 Twitter 的健康。我们承诺对这方面的进展向公众负责。”</p>
<p class="translated">“Twitter 结合使用机器学习和人工审查来裁定滥用报告以及它们是否违反了我们的规则。在评估虐待行为和确定适当的强制措施时，背景很重要。我们可能考虑的因素包括但不限于:行为是否针对个人或人群；虐待目标或旁观者已提交报告；并且该行为具有新闻价值并且符合合法的公共利益。Twitter 随后会向报告虐待行为的个人提供后续通知。我们还提供个人可以采取的其他措施的建议，以改善他或她的 Twitter 体验，例如使用屏蔽或静音功能。”</p>
<p class="translated">“关于你即将发表的报告，我要指出，为了对内容进行分类而提出的“有问题”内容的概念值得进一步讨论。不清楚你是如何定义或分类这些内容的，也不清楚你是否建议这些内容应该从 Twitter 上删除。我们努力建立全球可执行的规则，并已开始咨询公众，作为这一过程的一部分——这是业内的一种新方法。”</p>
<p class="translated">“正如许多民间社会团体所强调的，公司必须仔细界定其政策的范围，让用户清楚哪些内容是允许的，哪些是不允许的。我们欢迎进一步讨论您如何根据保护言论自由和确保政策起草清晰、严密的需要，将“有问题”定义为本研究的一部分。”</p></blockquote>
			</div>

			</div>    
</body>
</html>