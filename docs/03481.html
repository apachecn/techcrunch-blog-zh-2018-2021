<html>
<head>
<title>This robot learns its two-handed moves from human dexterity • TechCrunch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">这个机器人从人类的灵巧中学习双手动作 TechCrunch</h1>
<blockquote>原文：<a href="https://web.archive.org/web/https://techcrunch.com/2019/05/29/this-robot-learns-its-two-handed-moves-from-human-dexterity/">https://web.archive.org/web/https://techcrunch.com/2019/05/29/this-robot-learns-its-two-handed-moves-from-human-dexterity/</a></blockquote><div><div class="article-content">
				<p id="speakable-summary" class="translated">如果机器人真的要帮我们做家务或照顾我们的伤员和老人，它们至少需要两只手。但是使用两只手比我们想象的要难——所以这个机器人控制系统在尝试做同样的事情之前会向人类学习。</p>
<p class="translated">这项来自威斯康星大学麦迪逊分校的研究背后的想法不是从头开始建造双手机器人，而是简单地创建一个系统，它能够理解并执行我们人类不假思索地进行的相同类型的操作。</p>
<p class="translated">例如，当你需要打开一个罐子时，你用一只手抓住它，把它移动到合适的位置，然后用另一只手抓住盖子，把它拧开或打开。在这个基本的双手动作中有太多的事情正在进行，现在要求机器人自主完成它是没有希望的。但是机器人仍然可以对为什么在这种情况下进行这种类型的操作有一个大致的概念，并尽它所能去追求它。</p>
<p class="translated">研究人员首先让戴着动作捕捉设备的人执行各种模拟的日常任务，如叠杯子、打开容器并倒出内容物，以及拿起顶部平衡着其他东西的物品。所有这些数据——手去了哪里，它们如何互动等等——都被一个机器学习系统咀嚼和反复思考，该系统发现人们倾向于用手做四件事情中的一件:</p>
<ul>
<li class="translated"><strong>自我移交</strong>:这是你拿起一个物体，把它放在另一只手里，这样更容易把它放在它要去的地方，或者腾出第一只手来做其他事情。</li>
<li class="translated"><strong>单手固定</strong>:一只手牢牢抓住一个物体，而另一只手对它进行操作，比如打开盖子或者搅拌里面的东西。</li>
<li class="translated"><strong>固定偏移量</strong>:双手合力拿起某物并旋转或移动。</li>
<li class="translated"><strong>单手寻找</strong>:实际上并不是双手动作，而是故意让一只手不动作，而另一只手去寻找需要的对象或者执行自己的任务的原理。</li>
</ul>
<p class="translated">机器人将这些知识运用到工作中，不是做动作本身——同样，这些是当前人工智能无法执行的极其复杂的动作——而是解释人类控制器做出的运动。</p>
<p class="translated">你可能会认为，当一个人远程控制一个机器人时，它只会精确地模仿这个人的动作。在测试中，机器人这样做是为了提供一个基线，如果没有这些“双手动作”的知识，但其中许多根本不可能。</p>
<p class="translated">想想开瓶的例子。我们知道，当我们打开罐子时，我们必须用更强的握力握住一边，甚至可能必须用罐子手逆着打开的手的运动向后推。如果你试图用机械臂远程操作，这些信息就不再存在了，一只手可能会把罐子从另一只手的手里撞出来，或者因为另一只手不帮忙而抓不住罐子。</p>
<p class="translated">研究人员创建的系统可以识别上述四种行为中的一种何时发生，并采取措施确保它们成功。这意味着，例如，当他们一起提桶时，要意识到另一只手臂施加在另一只手上的压力。或者在另一个臂与盖子相互作用时为保持物体的臂提供额外的刚性。即使只使用一只手(“寻找”)，系统也知道它可以降低未使用的手的运动的优先级，并将更多的资源(无论是身体运动还是计算能力)用于工作的手。</p>
<p class="translated"><a href="https://web.archive.org/web/20221025223545/https://techcrunch.com/wp-content/uploads/2019/05/robogrip.gif"><img loading="lazy" class="aligncenter size-full wp-image-1833464" src="../Images/58bb384a0c9f997b818d4c837daef31b.png" alt="" data-original-src="https://web.archive.org/web/20221025223545im_/https://techcrunch.com/wp-content/uploads/2019/05/robogrip.gif"/>T2】</a></p>
<p class="translated">在演示视频中，这一知识显然大大提高了远程操作员尝试执行一系列任务的成功率，这些任务旨在模拟准备早餐:打(假)鸡蛋，搅拌和移动东西，拿起一个放有玻璃杯的托盘并保持水平。</p>
<p class="translated">当然，这一切或多或少仍是由人类来完成的——但人类的行为正在被增强，并被重新解释成某种不仅仅是简单的机械复制的东西。</p>
<p class="translated">自主完成这些任务还有很长的路要走，但像这样的研究为这项工作奠定了基础。在机器人试图像人类一样移动之前，它不仅要理解人类是如何移动的，还要理解他们为什么在特定环境下做特定的事情，此外，还有哪些重要的过程可能隐藏在明显的观察之外——比如规划手的路线，选择抓握位置等等。</p>
<p class="translated">麦迪逊队由丹尼尔·拉基塔带领；他们描述该系统的论文发表在《科学机器人学》杂志上。</p>
			</div>

			</div>    
</body>
</html>