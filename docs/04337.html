<html>
<head>
<title>Bias in AI: A problem recognized but still unresolved </title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">人工智能中的偏见:一个已认识但仍未解决的问题</h1>
<blockquote>原文：<a href="https://web.archive.org/web/https://techcrunch.com/2019/07/25/bias-in-ai-a-problem-recognized-but-still-unresolved/">https://web.archive.org/web/https://techcrunch.com/2019/07/25/bias-in-ai-a-problem-recognized-but-still-unresolved/</a></blockquote><div><div class="article-content">
				<div class="article__contributor-byline-wrapper">
<div class="article__contributor-byline">
	

		
	
		<div class="contributor-byline__more-articles">
		<span class="more-articles-title">More posts by this contributor</span>
		
	</div>
	</div>
</div><p id="speakable-summary" class="translated">有人称赞这项技术解决了人类一些最严重的问题，也有人将人工智能妖魔化为世界上最大的生存威胁。当然，这是光谱的两端，人工智能无疑为未来带来了令人兴奋的机会，也带来了有待克服的挑战性问题。</p>
<p class="translated">近年来吸引媒体关注的一个问题是人工智能中的偏见前景。是我两年多前在 TechCrunch <a href="https://web.archive.org/web/20230320090458/https://techcrunch.com/2017/01/18/tyrant-in-the-code/" target="_blank" rel="noopener noreferrer" data-saferedirecturl="https://www.google.com/url?q=https://techcrunch.com/2017/01/18/tyrant-in-the-code/&amp;source=gmail&amp;ust=1560869715176000&amp;usg=AFQjCNEYYWMuerGnefqE9rswuuyvq8xgdA">(代码中的暴君)</a>上写过的一个话题。辩论正在激烈进行。</p>
<p class="translated">当时，当<a href="https://web.archive.org/web/20230320090458/https://www.buzzfeed.com/fionarutherford/heres-why-some-people-think-googles-results-are-racist" target="_blank" rel="noopener noreferrer" data-saferedirecturl="https://www.google.com/url?q=https://www.buzzfeed.com/fionarutherford/heres-why-some-people-think-googles-results-are-racist&amp;source=gmail&amp;ust=1560869715177000&amp;usg=AFQjCNFjWGIXUErAtoXzQW3Qw8QccihT7g">研究</a>显示当用户在网上搜索“手”时，图像结果几乎全是白色时，谷歌受到了抨击；但当搜索“黑手”时，这些图像是更具贬义的描述，包括一只伸出手来帮助一只黑手，或在地里干活的黑手。这是一个令人震惊的发现，导致有人声称，人工智能技术非但不会弥合社会分歧，反而会使分歧永久化。</p>
<p class="translated">正如我两年前断言的那样，这种情况可能发生也就不足为奇了。至少在 2017 年，美国设计人工智能算法的绝大多数人是白人男性。虽然这并不意味着这些人对少数民族有偏见，但他们在他们创造的人工智能中传递他们自然、无意识的偏见是有道理的。</p>
<p class="translated">不仅仅是谷歌算法面临有偏见的人工智能的风险。随着该技术在各个行业变得越来越普遍，消除技术中的任何偏见将变得越来越重要。</p>
<h2 class="translated">理解问题</h2>
<p class="translated">两年前，人工智能在许多行业和应用中确实是重要和不可或缺的，但可以预见的是，它的重要性自那以后一直在增加。人工智能系统现在被用来帮助招聘人员识别合适的候选人，帮助 T2 贷款承销商决定是否借钱给客户，甚至帮助 T4 法官考虑一名罪犯是否会再次犯罪。</p>
<p class="translated">当然，数据当然可以帮助人类利用人工智能和数据做出更明智的决定，但如果人工智能技术有偏见，结果也会如此。如果我们继续将人工智能技术的未来托付给一个非多样化的群体，那么社会中最弱势的成员可能会在找工作、获得贷款和接受司法系统的公平审判等方面处于不利地位。</p>
<p>	</p><div class="article-block block--pullout block--right">
		<blockquote class="translated">人工智能是一场革命，无论是否需要，它都将继续。</blockquote>
	</div>
	
<p class="translated">幸运的是，近年来，围绕人工智能偏见的问题已经浮出水面，越来越多的有影响力的人物，组织和政治机构正在认真研究如何处理这个问题。</p>
<p class="translated">AI Now Institute 就是这样一个研究人工智能技术的社会影响的组织。人工智能由研究科学家凯特·克劳福德(Kate Crawford)和梅雷迪思·惠特克(Meredith Whittaker)于 2017 年推出，现在专注于人工智能对人权和劳动力的影响，以及如何安全地集成人工智能和如何避免技术中的偏见。</p>
<p class="translated">去年 5 月，欧盟出台了《通用数据保护条例》(GDPR)，这是一套让欧盟公民对如何在线使用他们的数据有更多控制权的规则。虽然它不会直接挑战人工智能技术中的偏见，但它会迫使欧洲组织(或任何有欧洲客户的组织)在使用算法时更加透明。这将给公司带来额外的压力，以确保他们对自己使用的人工智能的来源有信心。</p>
<p class="translated">虽然美国还没有一套类似的关于数据使用和人工智能的法规，但在 2017 年 12 月，<a href="https://web.archive.org/web/20230320090458/https://www.propublica.org/article/new-york-city-moves-to-create-accountability-for-algorithms" target="_blank" rel="noopener noreferrer" data-saferedirecturl="https://www.google.com/url?q=https://www.propublica.org/article/new-york-city-moves-to-create-accountability-for-algorithms&amp;source=gmail&amp;ust=1560869715177000&amp;usg=AFQjCNE4pLX2iaJm8IUc-GU18krdyklJzQ">纽约市议会和市长通过了一项法案</a>，呼吁提高人工智能的透明度，因为有报道称该技术在刑事判决中造成了种族偏见。</p>
<p class="translated">尽管研究团体和政府机构对有偏见的人工智能可能在社会中发挥的潜在破坏性作用感兴趣，但责任主要落在创造技术的企业身上，以及他们是否准备好解决问题的核心。幸运的是，一些最大的科技公司，包括那些过去被指责忽视人工智能偏见问题的公司，正在采取措施解决这个问题。</p>
<p class="translated">例如，微软现在正在雇佣艺术家、哲学家和有创造力的作家来训练人工智能机器人，让它们知道细微语言的该做什么和不该做什么，比如不要使用不恰当的俚语，或者无意中发表种族主义或性别歧视的言论。<a href="https://web.archive.org/web/20230320090458/https://www.research.ibm.com/5-in-5/ai-and-bias/" target="_blank" rel="noopener noreferrer" data-saferedirecturl="https://www.google.com/url?q=https://www.research.ibm.com/5-in-5/ai-and-bias/&amp;source=gmail&amp;ust=1560869715177000&amp;usg=AFQjCNH-iYD5YNMC3XqrbhHZHfOm79teSw"> IBM </a>正试图通过应用独立的偏见评级来确定其人工智能系统的公平性，从而减轻其人工智能机器中的偏见。去年 6 月，<a href="https://web.archive.org/web/20230320090458/https://www.standard.co.uk/tech/bias-in-ai-google-head-of-ethical-machine-learning-a3986256.html" target="_blank" rel="noopener noreferrer" data-saferedirecturl="https://www.google.com/url?q=https://www.standard.co.uk/tech/bias-in-ai-google-head-of-ethical-machine-learning-a3986256.html&amp;source=gmail&amp;ust=1560869715177000&amp;usg=AFQjCNFUzpNPx70dialy48KEwW0QXtvtsQ">谷歌首席执行官桑德尔·皮帅发布了一套人工智能原则</a>，旨在确保该公司的工作或研究不会在其算法中产生或强化偏见。</p>
<h2 class="translated">人工智能中的人口统计工作</h2>
<p class="translated">解决人工智能中的偏见确实需要个人、组织和政府机构认真看待问题的根源。但这些根源通常是最初创建人工智能服务的人。正如我两年前在《代码中的暴君》一书中所假设的那样，任何与右手剪刀、账本和开罐器斗争过的左撇子都会知道，发明往往偏爱它们的创造者。人工智能系统也是如此。</p>
<p class="translated">来自美国劳工统计局的新数据显示，编写人工智能程序的专业人士仍然主要是白人男性。去年八月由<a href="https://web.archive.org/web/20230320090458/https://www.wired.com/story/artificial-intelligence-researchers-gender-imbalance/" target="_blank" rel="noopener noreferrer" data-saferedirecturl="https://www.google.com/url?q=https://www.wired.com/story/artificial-intelligence-researchers-gender-imbalance/&amp;source=gmail&amp;ust=1560869715177000&amp;usg=AFQjCNFXKEcJgh82lBBNnApgQn3olWxPMA"> Wired 和 Element AI </a>进行的一项研究发现，只有 12%的领先机器学习研究人员是女性。</p>
<p class="translated">这并不是一个被创造人工智能系统的技术公司完全忽视的问题。例如，英特尔正在采取积极措施，提高公司技术职位的性别多样性。最近的数据表明，女性在英特尔的技术职位中占了<a href="https://web.archive.org/web/20230320090458/https://www.bizjournals.com/denver/bizwomen/news/profiles-strategies/2019/02/the-women-reshaping-intels-ai-mission.html?page=all" target="_blank" rel="noopener noreferrer" data-saferedirecturl="https://www.google.com/url?q=https://www.bizjournals.com/denver/bizwomen/news/profiles-strategies/2019/02/the-women-reshaping-intels-ai-mission.html?page%3Dall&amp;source=gmail&amp;ust=1560869715177000&amp;usg=AFQjCNHfyRBpfyUftiY6kzdBv2pMd1ErHg">24%</a>——远远高于行业平均水平。谷歌正在资助一个针对下一代人工智能领导者的人工智能夏令营，以扩大其对年轻女性和在技术领域代表性不足的少数族裔的影响。</p>
<p class="translated">然而，统计数据显示，如果人工智能要达到消除技术偏见所需的多样性水平，还有很长的路要走。尽管一些公司和个人做出了努力，但科技公司仍然是白人和男性占压倒性优势。</p>
<h2 class="translated">解决人工智能中的偏差问题</h2>
<p class="translated">当然，改善主要人工智能公司内部的多样性将大大有助于解决技术中的偏见问题。负责分发影响社会的人工智能系统的企业领导人将需要提供公共透明度，以便可以监测偏见，将道德标准纳入技术，并更好地了解算法应该针对谁。</p>
<p>	</p><div class="article-block block--pullout block--right">
		<blockquote class="translated">政府和商界领袖都有一些严肃的问题需要思考。</blockquote>
	</div>
	
<p class="translated">但是，如果没有政府机构的监管，这些类型的解决方案可能会太慢，如果有的话。虽然欧盟已经建立了 GDPR，在许多方面缓和了人工智能的偏见，但没有强烈的迹象表明美国会很快效仿。</p>
<p class="translated">在私人研究人员和智库的帮助下，政府正在朝着这个方向快速前进，并试图解决如何监管算法的问题。此外，像 T2 脸书这样的公司也声称监管是有益的。然而，对用户生成内容平台的高监管要求可能会帮助像脸书这样的公司，因为它几乎不可能竞争进入市场的新创业公司。</p>
<p class="translated">问题是，政府干预的理想水平是什么，不会阻碍创新？</p>
<p class="translated">企业家经常声称监管是创新的敌人，对于这样一种可能改变游戏规则的相对新生的技术，应该不惜一切代价避免任何障碍。然而，人工智能是一场革命，无论是否需要，它都将继续下去。它将继续改变数十亿人的生活，因此它显然需要朝着道德、公正的方向发展。</p>
<p class="translated">政府和商界领袖都有一些严肃的问题需要思考，但没有太多时间去思考。人工智能是一项发展迅速的技术，它不会等待犹豫不决。如果允许创新不受约束地继续下去，没有什么道德准则，没有多元化的创作者群体，结果可能会导致美国和世界范围内的分歧加深。</p>
			</div>

			</div>    
</body>
</html>