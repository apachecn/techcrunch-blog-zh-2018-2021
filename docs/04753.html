<html>
<head>
<title>The risks of amoral AI • TechCrunch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">不道德的风险</h1>
<blockquote>原文：<a href="https://web.archive.org/web/https://techcrunch.com/2019/08/25/the-risks-of-amoral-a-i/">https://web.archive.org/web/https://techcrunch.com/2019/08/25/the-risks-of-amoral-a-i/</a></blockquote><div><div class="article-content">
				<div class="article__contributor-byline-wrapper">
<div class="article__contributor-byline">
	

		<div class="contributor-byline__bio"><p class="translated">凯尔·登特是</p><a href="https://web.archive.org/web/20221208153753/http://www.parc.com/">PARC, a Xerox Company</a><p class="translated">，关注人与技术之间的相互作用。他还领导 PARC 大学的伦理审查委员会。</p></div>
	
	</div>
</div><p class="translated"><span class="featured__span-first-words">人工智能现在被用于对现实世界中的生活、生计和互动做出决定，给人们带来了真正的风险。</span></p>
<p class="translated">我们曾经都是怀疑论者。不久前，传统智慧认为机器智能显示出巨大的前景，但它始终只是几年之遥。今天，人们坚信未来已经到来。</p>
<p class="translated">汽车(有时和在特定条件下)自动驾驶，软件在国际象棋和围棋等游戏中击败人类，这并不奇怪。不能怪人家印象深刻。</p>
<p class="translated">但桌游，即使是复杂的桌游，也与现实生活的混乱和不确定性相去甚远，自动驾驶汽车实际上仍然没有与我们共享道路(至少没有一些灾难性的失败)。</p>
<p class="translated">人工智能正被用于数量惊人的应用中，对工作表现、招聘、贷款和<a href="https://web.archive.org/web/20221208153753/https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing">刑事司法</a>等做出判断。大多数人并没有意识到这些判断中的潜在风险。他们应该是。人们普遍认为技术本质上是中立的——即使在许多开发人工智能解决方案的人中也是如此。但是人工智能开发者做决定，选择 trade-oﬀs，而不是 aﬀect 结果。开发人员在技术中嵌入了道德选择，但没有从这些方面考虑他们的决定。</p>
<p class="translated">这些 trade-oﬀs 通常是技术性的和微妙的，下游的含义在决策时并不总是显而易见的。</p>
<p class="translated">亚利桑那州坦佩的<a href="https://web.archive.org/web/20221208153753/https://techcrunch.com/2018/05/24/uber-in-fatal-crash-detected-pedestrian-but-had-emergency-braking-disabled/" data-saferedirecturl="https://www.google.com/url?q=https://en.wikipedia.org/wiki/Death_of_Elaine_Herzberg&amp;source=gmail&amp;ust=1566498453743000&amp;usg=AFQjCNEwyDhVdABlY8SdAE6-vdTfr_SiMA">致命优步事故</a>是一个(不微妙)但很好的说明性例子，很容易看出它是如何发生的。</p>
<p class="translated">自动驾驶汽车系统实际上及时检测到行人并停下来，但开发人员调整了紧急制动系统，以利于不过度制动，平衡颠簸驾驶和安全之间的 trade-oﬀ。优步开发商选择了更具商业可行性的选择。最终，自动驾驶技术将会改善到既安全又平稳的程度，但在这种情况发生之前，我们会让自动驾驶汽车上路吗？利益集团正努力推动它们立即上路。</p>

<p class="translated">身体风险构成了明显的危险，但自动决策系统也带来了实实在在的伤害。事实上，人工智能确实有造福世界的潜力。理想情况下，我们会减轻负面影响，以最小的伤害获得好处。</p>
<p class="translated">一个显著的风险是，我们以减少个人人权为代价来推进人工智能技术的使用。我们已经看到这种情况发生了。一个重要的例子是，当涉及到 AI 工具时，对司法判决提起上诉的权利被削弱。在许多其他情况下，个人甚至不知道不雇佣、不晋升或不贷款给他们的选择是由统计算法告知的。<strong> </strong></p>
<p class="translated"><strong>买家当心</strong></p>
<p class="translated">技术的购买者处于不利地位，因为他们对技术的了解比销售者少得多。在很大程度上，决策者不具备评估智能系统的能力。在经济方面，信息不对称使人工智能开发者比那些可能使用它的人处于更强大的地位。(旁注:AI 决策的主体一般完全没有权力。)AI 的本质就是你单纯的相信(或者不相信)它做出的决定。你不能问技术它为什么决定某事，或者它是否考虑了其他选择，或者提出假设来探索你所问问题的变化。考虑到目前对技术的信任，供应商承诺以更便宜、更快捷的方式完成工作可能非常诱人。</p>
<p class="translated">到目前为止，我们作为一个社会还没有一种方法来评估算法的价值和它们给社会带来的成本。即使政府实体决定采用新的人工智能解决方案，也很少有公开讨论。更糟糕的是，关于用于训练系统的数据以及其加权方案、模型选择和供应商在开发软件时做出的其他选择的信息被视为商业秘密，因此不能进行讨论。</p>
<p/><div id="attachment_1846125" class="wp-caption aligncenter"><img aria-describedby="caption-attachment-1846125" decoding="async" loading="lazy" class="wp-image-1846125 size-large" title="Replacing people with robots - Businessman" src="../Images/57624cce0cc950d5bbc272af1b5607e7.png" alt="" data-original-src="https://web.archive.org/web/20221208153753im_/https://techcrunch.com/wp-content/uploads/2019/06/GettyImages-939559562-e1560947249875.jpg?w=516"/><p id="caption-attachment-1846125" class="wp-caption-text translated">图片来自 Getty Images / sorbetto</p></div>
<p class="translated">《耶鲁法律与技术杂志》发表了 Robert Brauneis 和 Ellen P. Goodman 的一篇论文，他们在论文中描述了他们的 eﬀorts，以测试政府采用预测算法的数据分析工具的透明度。他们向各种公共机构提交了 42 份关于使用决策支持工具的公开记录请求。</p>
<p class="translated">他们的“具体目标是评估公开记录流程是否能让公民发现这些算法体现了什么样的政策判断，并评估它们的效用和公平性”。几乎所有参与的机构都不愿意或不能提供信息，这些信息可能导致人们理解算法如何决定公民的命运。政府记录保存是最大的问题之一，但公司咄咄逼人的商业秘密和机密声明也是一个重要因素。</p>
<p class="translated">使用数据驱动的风险评估工具可能非常有用，特别是在识别可从减刑中受益的低风险个人的情况下。减刑或免刑减轻了监狱系统的压力，也有利于个人、其家庭和社区。尽管有可能的好处，但如果这些工具干扰了宪法规定的正当程序权，就不值得冒这个险。</p>
<p class="translated">我们所有人都有权质疑司法程序和许多其他情况下使用的信息的准确性和相关性。对威斯康星州的公民来说，不幸的是，该州最高法院在 2016 年裁定，公司的利益高于被告的正当程序权。</p>
<h2 class="translated">情人眼里出西施</h2>
<p class="translated">当然，人类的判断也有偏差。事实上，职业文化必须进化才能解决这个问题。例如，法官努力将他们的偏见从他们的判决中分离出来，并且有质疑司法判决公正性的程序。</p>
<p class="translated">在美国，1968 年公平住房法案(Fair Housing Act)获得通过，以确保房地产专业人士在开展业务时不会歧视客户。科技公司没有这样的文化。最近的新闻显示正好相反。对于个人 AI 开发人员来说，重点是让算法以高精度正确，无论他们在建模中假设什么精度定义。</p>
<p class="translated">我最近听了一个播客，其中的对话想知道关于人工智能偏见的谈论是否没有将机器置于比人类更高的 diﬀerent 标准——似乎表明机器在一些想象的与人类的竞争中处于劣势。</p>
<p class="translated">作为真正的技术信徒，主持人和嘉宾最终得出结论，一旦人工智能研究人员解决了机器偏见问题，我们将有一个新的，甚至更好的标准供人类遵守，到那时，机器可以教人类如何避免偏见。这意味着有一个客观的答案，虽然我们人类一直在努力寻找它，但机器可以为我们指明道路。事实是，在许多情况下，关于公平的含义存在着相互矛盾的概念。</p>
<p class="translated">在过去的几年里，已经有一些研究论文从统计学和数学的角度解决了公平问题。例如，其中一篇论文正式规定了一些基本标准，以确定一项决定是否公平。</p>
<p class="translated">在大多数情况下，diﬀering 关于公平的概念不仅仅是 diﬀerent 式的，而且实际上是不相容的。可以被称为公平的单一客观解决方案根本不存在，这使得经过统计训练的机器无法回答这些问题。从这个角度来看，一场关于机器给人类上公平课的对话听起来更像是一场荒谬的戏剧，而不是一场关于所涉及问题的据称深思熟虑的对话。</p>
<p/><div id="attachment_1759224" class="wp-caption aligncenter"><img aria-describedby="caption-attachment-1759224" decoding="async" loading="lazy" class="breakout wp-image-1759224 size-full" title="facebook-thumbs-down-1" src="../Images/d81c1728e00d39070018685fc469e255.png" alt="" srcset="https://web.archive.org/web/20221208153753im_/https://techcrunch.com/wp-content/uploads/2018/12/facebook-thumbs-down-11.png 730w, https://web.archive.org/web/20221208153753im_/https://techcrunch.com/wp-content/uploads/2018/12/facebook-thumbs-down-11.png?resize=150,84 150w, https://web.archive.org/web/20221208153753im_/https://techcrunch.com/wp-content/uploads/2018/12/facebook-thumbs-down-11.png?resize=300,168 300w, https://web.archive.org/web/20221208153753im_/https://techcrunch.com/wp-content/uploads/2018/12/facebook-thumbs-down-11.png?resize=680,382 680w, https://web.archive.org/web/20221208153753im_/https://techcrunch.com/wp-content/uploads/2018/12/facebook-thumbs-down-11.png?resize=50,28 50w" sizes="(max-width: 730px) 100vw, 730px" data-original-src="https://web.archive.org/web/20221208153753im_/https://techcrunch.com/wp-content/uploads/2018/12/facebook-thumbs-down-11.png"/><p id="caption-attachment-1759224" class="wp-caption-text translated">图片由 TechCrunch/布莱斯·德宾提供</p></div>
<p class="translated">当有偏见的问题时，讨论是必要的。例如，在刑事判决、发放贷款、工作和大学机会等背景下，公平意味着什么还没有得到解决，不幸的是还包含政治因素。我们被要求加入人工智能可以以某种方式去政治化这些问题的幻觉。事实是，这项技术体现了一种特殊的姿态，但我们不知道它是什么。</p>
<p class="translated">埋头专注于算法的技术专家正在决定重要的结构性问题并做出政策选择。这消除了集体对话，并减少了 oﬀ从其他角度的投入。社会学家、历史学家、政治学家，尤其是社区内的所有利益相关者，都可以为这场辩论做出很多贡献。将人工智能应用于这些棘手的问题，描绘了一幅试图为 diﬃcult 问题提供非政治解决方案的科学外衣。<strong> </strong></p>
<h2 class="translated">谁来监视(人工智能)观察者？</h2>
<p class="translated">当前采用人工智能解决方案趋势的一个主要驱动因素是，开发人工智能的公司不承担使用人工智能的负面外部性。通常，我们通过政府监管来解决这种情况。例如，工业污染受到限制，因为它会给社会带来未来的成本。我们还利用监管来保护可能受到伤害的个人。</p>
<p class="translated">这两种潜在的负面后果都存在于我们目前对人工智能的使用中。对于自动驾驶汽车，已经有监管机构参与其中，因此我们可以期待一场关于何时以及以何种方式使用人工智能驾驶汽车的公开对话。AI 的其他用途呢？目前，除了纽约市的一些行动外，关于人工智能的使用没有任何监管。对于技术的使用者和自动决策的主体来说，算法责任的最基本保证都不能保证。</p>
<p/><div id="attachment_1861760" class="wp-caption aligncenter"><img aria-describedby="caption-attachment-1861760" decoding="async" loading="lazy" class="breakout wp-image-1861760 size-full" title="Advantages and disadvantages of modern technologies. A pair of white robots dressed like an angel and a devil / flat editable vector illustration, clip art" src="../Images/be953046ddf0e8cd536bafe9e5caa897.png" alt="GettyImages 823303786" srcset="https://web.archive.org/web/20221208153753im_/https://techcrunch.com/wp-content/uploads/2019/07/GettyImages-823303786.jpg 1732w, https://web.archive.org/web/20221208153753im_/https://techcrunch.com/wp-content/uploads/2019/07/GettyImages-823303786.jpg?resize=150,150 150w, https://web.archive.org/web/20221208153753im_/https://techcrunch.com/wp-content/uploads/2019/07/GettyImages-823303786.jpg?resize=300,300 300w, https://web.archive.org/web/20221208153753im_/https://techcrunch.com/wp-content/uploads/2019/07/GettyImages-823303786.jpg?resize=768,768 768w, https://web.archive.org/web/20221208153753im_/https://techcrunch.com/wp-content/uploads/2019/07/GettyImages-823303786.jpg?resize=680,680 680w, https://web.archive.org/web/20221208153753im_/https://techcrunch.com/wp-content/uploads/2019/07/GettyImages-823303786.jpg?resize=1536,1536 1536w, https://web.archive.org/web/20221208153753im_/https://techcrunch.com/wp-content/uploads/2019/07/GettyImages-823303786.jpg?resize=1200,1200 1200w, https://web.archive.org/web/20221208153753im_/https://techcrunch.com/wp-content/uploads/2019/07/GettyImages-823303786.jpg?resize=32,32 32w, https://web.archive.org/web/20221208153753im_/https://techcrunch.com/wp-content/uploads/2019/07/GettyImages-823303786.jpg?resize=50,50 50w, https://web.archive.org/web/20221208153753im_/https://techcrunch.com/wp-content/uploads/2019/07/GettyImages-823303786.jpg?resize=64,64 64w, https://web.archive.org/web/20221208153753im_/https://techcrunch.com/wp-content/uploads/2019/07/GettyImages-823303786.jpg?resize=96,96 96w, https://web.archive.org/web/20221208153753im_/https://techcrunch.com/wp-content/uploads/2019/07/GettyImages-823303786.jpg?resize=128,128 128w" sizes="(max-width: 1732px) 100vw, 1732px" data-original-src="https://web.archive.org/web/20221208153753im_/https://techcrunch.com/wp-content/uploads/2019/07/GettyImages-823303786.jpg"/><p id="caption-attachment-1861760" class="wp-caption-text translated">图片来自 Getty Images / nadia_bormotova</p></div>
<p class="translated">不幸的是，我们不能让公司自己监管自己。脸书的口号“快速行动，打破常规”已经过时，但这种思维模式和文化在整个硅谷依然存在。一种做你认为最好的事情，然后道歉的态度继续占据主导地位。</p>
<p class="translated">这显然是 eﬀective 在构建向消费者追加销售或将乘客与司机联系起来的系统时的做法。当您对 aﬀecting 人民生活做出决定时，这变得完全不可接受。即使是善意的，编写代码的研究人员和开发人员也没有经过培训，或者，冒着 oﬀending 一些优秀同事的风险，没有考虑这些问题的倾向。</p>
<p class="translated">我见过太多的研究人员对人类影响表现出惊人的漠不关心。我最近参加了一个在硅谷外举行的创新会议。其中一个演示包括一段篡改过的视频，视频中一个非常著名的人发表了一个从未实际发生过的演讲。对视频的操控完全察觉不到。</p>
<p class="translated">当研究人员被问及欺骗性技术的含义时，她对这个问题不屑一顾。她的回答基本上是，“我制造技术，然后把那些问题留给社会科学家去解决。”这只是我从许多研究人员那里看到的最糟糕的例子之一，他们并没有把这些问题放在他们的雷达上。我认为要求计算机科学家双修道德哲学专业是不现实的，但是缺乏关注是惊人的。</p>
<p class="translated">最近我们了解到，亚马逊放弃了一项内部技术，他们一直在测试这项技术，以从申请人中选择最佳简历。亚马逊发现，在 eﬀect，他们创建的系统发展出了对男性候选人的偏好，对申请的女性不利。在这种情况下，亚马逊受到 suﬃciently 的激励，以确保他们自己的技术尽可能像 eﬀectively 一样工作，但其他公司会保持警惕吗？</p>
<p class="translated">事实上，路透社报道称，其他公司正在愉快地推进人工智能招聘。销售此类技术的第三方供应商实际上没有动力测试它是否没有偏见，除非客户有此要求，正如我提到的，决策者大多没有资格进行这种对话。同样，人的偏见在招聘中也起了作用。但是公司可以也应该处理这个问题。</p>
<p class="translated">有了机器学习，他们无法确定系统可能会学习到哪些歧视性特征。如果没有市场力量，除非企业被迫在公平至关重要的领域对不透明技术的开发和使用保持透明，否则这种情况不会发生。</p>
<p class="translated">问责制和透明度对于在现实世界应用中安全使用人工智能至关重要。法规可能要求获得有关该技术的基本信息。由于没有一个解决方案是完全准确的，该法规应该允许采用者了解错误的 eﬀects。错误相对较小还是较大？优步使用人工智能杀死了一名行人。其他应用中的最坏情况有多糟糕？算法是如何训练的？培训使用了哪些数据，如何评估这些数据以确定其是否符合预期目的？它是否真正代表了所考虑的人民？它包含偏见吗？只有获得此类信息，利益相关方才能对适当的风险和 trade-oﬀs.做出明智的决策</p>
<p class="translated">在这一点上，我们可能不得不面对这样一个事实，即我们目前对人工智能的使用超出了它的能力，安全地使用它需要比现在更多的思考。</p>
			</div>

			</div>    
</body>
</html>