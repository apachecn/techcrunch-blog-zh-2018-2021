<html>
<head>
<title>Apple launches Deep Fusion feature in beta on iPhone 11 and iPhone 11 Pro </title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">苹果在iPhone 11和iPhone 11 Pro上推出测试版深度融合功能</h1>
<blockquote>原文：<a href="https://web.archive.org/web/https://techcrunch.com/2019/10/01/apple-launches-deep-fusion-feature-in-beta-on-iphone-11-and-iphone-11-pro/">https://web.archive.org/web/https://techcrunch.com/2019/10/01/apple-launches-deep-fusion-feature-in-beta-on-iphone-11-and-iphone-11-pro/</a></blockquote><div><div class="article-content">
				<p id="speakable-summary" class="translated"><span> <strong>下午5:02更新PT: </strong> <em>包含深度融合的开发者beta版终究不会在今天发货。没有宣布日期；只是“来了”</em> </span></p>
<p class="translated"><span>苹果今天在iOS上发布了新的深度融合功能的早期版本，并为测试版用户提供了软件更新。深度融合是一种在像素级别将多次曝光混合在一起的技术，与使用标准HDR成像相比，它可以为用户提供更高层次的细节，特别是在具有非常复杂纹理的图像中，如皮肤、衣服或箔片。</span></p>
<p class="translated"><span>今天发布的开发者测试版支持iPhone 11，其中深度融合将改善宽相机上拍摄的照片，以及iPhone 11 Pro和Pro Max，其中它将支持长焦和广角，但不支持超宽镜头。</span></p>
<p class="translated"><span>据苹果公司称，深度融合需要A13，不会在任何旧款iPhones上提供。</span></p>
<p class="translated"><span>正如我在对iPhone 11 Pro的评论中广泛谈到的那样，苹果在iPhone中的“摄像头”实际上是镜头和传感器的集合，由运行在专用硬件上的专用机器学习软件进行积极处理。实际上，一个机器学习相机。</span></p>
<p class="translated"><span>深度融合是一项迷人的技术，它将苹果的摄影哲学作为一种计算过程扩展到了下一个逻辑前沿。截至iPhone 7，苹果一直在混合广角镜头和长焦镜头的输出，以提供最佳效果。这个过程在用户没有意识到的情况下发生了。</span></p>
<p class="translated"><img decoding="async" class="aligncenter wp-image-1889482 size-full" title="E020289B-A902-47A8-A2AD-F6B31B16BEC8" src="../Images/175e13301e50d6bd01526adc2c5ca3df.png" alt="E020289B A902 47A8 A2AD F6B31B16BEC8" srcset="https://web.archive.org/web/20230406123721im_/https://techcrunch.com/wp-content/uploads/2019/10/E020289B-A902-47A8-A2AD-F6B31B16BEC8-e1569951062496.jpeg 4032w, https://web.archive.org/web/20230406123721im_/https://techcrunch.com/wp-content/uploads/2019/10/E020289B-A902-47A8-A2AD-F6B31B16BEC8-e1569951062496.jpeg?resize=150,113 150w, https://web.archive.org/web/20230406123721im_/https://techcrunch.com/wp-content/uploads/2019/10/E020289B-A902-47A8-A2AD-F6B31B16BEC8-e1569951062496.jpeg?resize=300,225 300w, https://web.archive.org/web/20230406123721im_/https://techcrunch.com/wp-content/uploads/2019/10/E020289B-A902-47A8-A2AD-F6B31B16BEC8-e1569951062496.jpeg?resize=768,576 768w, https://web.archive.org/web/20230406123721im_/https://techcrunch.com/wp-content/uploads/2019/10/E020289B-A902-47A8-A2AD-F6B31B16BEC8-e1569951062496.jpeg?resize=680,510 680w, https://web.archive.org/web/20230406123721im_/https://techcrunch.com/wp-content/uploads/2019/10/E020289B-A902-47A8-A2AD-F6B31B16BEC8-e1569951062496.jpeg?resize=1536,1152 1536w, https://web.archive.org/web/20230406123721im_/https://techcrunch.com/wp-content/uploads/2019/10/E020289B-A902-47A8-A2AD-F6B31B16BEC8-e1569951062496.jpeg?resize=2048,1536 2048w, https://web.archive.org/web/20230406123721im_/https://techcrunch.com/wp-content/uploads/2019/10/E020289B-A902-47A8-A2AD-F6B31B16BEC8-e1569951062496.jpeg?resize=1200,900 1200w, https://web.archive.org/web/20230406123721im_/https://techcrunch.com/wp-content/uploads/2019/10/E020289B-A902-47A8-A2AD-F6B31B16BEC8-e1569951062496.jpeg?resize=50,38 50w" sizes="(max-width: 4032px) 100vw, 4032px" data-original-src="https://web.archive.org/web/20230406123721im_/https://techcrunch.com/wp-content/uploads/2019/10/E020289B-A902-47A8-A2AD-F6B31B16BEC8-e1569951062496.jpeg"/></p>
<p class="translated"><span>深度融合继续以这种方式进行。它会自动对在特定情况下拍摄的图像生效。</span></p>
<p class="translated"><span>在广角镜头下，它将在大约10勒克斯的地板上开始活动，这是夜间模式开始的地方。它处于活动状态的场景范围的顶部是可变的，取决于光源。在长焦镜头上，它将在除最亮的情况下以外的所有情况下活跃，智能HDR将接管，由于高光的丰富，提供更好的结果。</span></p>
<p class="translated"><span>苹果公司提供了几张展示深度融合的图片样本，我已经在这里嵌入了。他们还没有提供任何非DF的例子，但是一旦测试版出来，人们安装它，我们就会看到。</span></p>
<p class="translated"><span>深度融合是这样工作的:</span></p>
<p class="translated">相机以负EV值拍摄“短”帧。基本上是一个比你想要的稍微暗一点的图像，并且从这个画面中去除了锐度。然后，它拍摄3张普通EV0照片和一个“长”EV+帧，登记对齐并混合在一起。</p>
<p class="translated"><span>这产生了两张12MP的照片——相当于24MP的数据——它们被组合成一张12MP的结果照片。这两者的结合是使用4个独立的神经网络完成的，这些神经网络考虑了苹果相机传感器的噪声特性以及图像中的主题。</span></p>
<p class="translated"><span>这种结合是在逐像素的基础上完成的。一次提取一个像素，以产生整个图像的最佳组合。机器学习模型查看图像的上下文，以确定它们在图像频谱上的位置。天空和其他大致相似的低频区域，中频区域中的肤色和高频项目，如衣服、箔片等。</span></p>
<p class="translated">然后，系统根据比例从一幅或另一幅图像中提取结构和色调。</p>
<p class="translated"><span>苹果公司表示，总体结果是，在移动对象的边缘，皮肤过渡更好，服装细节更好，清晰度更高。</span></p>
<p class="translated"><span>目前没有办法关闭深度融合过程，但是，因为新相机的“过度裁剪”功能使用超宽镜头，所以要查看图像之间的差异，需要一个小“黑客”来打开它，这将禁用深度融合，因为它不使用超宽镜头。</span></p>
<p class="translated"><span>深度融合过程需要大约1秒的处理时间。如果您快速拍摄，然后点击图像的预览，图像可能需要大约半秒钟才能更新到新版本。大多数人根本不会注意到这个过程的发生。</span></p>
<p class="translated"><span>至于它是如何运作的IRL？我们将进行测试，并在Deep Fusion可用时回复您。</span></p>
			</div>

			</div>    
</body>
</html>