<html>
<head>
<title>AI researchers condemn predictive crime software, citing racial bias and flawed methods </title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">人工智能研究人员谴责预测犯罪软件，引用种族偏见和有缺陷的方法</h1>
<blockquote>原文：<a href="https://web.archive.org/web/https://techcrunch.com/2020/06/23/ai-crime-prediction-open-letter-springer/">https://web.archive.org/web/https://techcrunch.com/2020/06/23/ai-crime-prediction-open-letter-springer/</a></blockquote><div><div class="article-content">
				<p id="speakable-summary" class="translated">一个由2000多名人工智能研究人员、学者和专家组成的集体大声反对即将发表的声称使用神经网络“预测犯罪”的研究在撰写本文时，脸书、谷歌和微软等公司的50多名人工智能员工已经签署了一封公开信，反对这项研究，并恳求出版商重新考虑。</p>
<p class="translated">《自然》的出版商斯普林格即将出版的系列丛书将突出这项有争议的研究。它的作者令人震惊地声称，他们的自动面部识别软件可以预测一个人是否会成为罪犯，并引用了这种工作在预测性警务的执法应用中的效用。</p>
<p class="translated">哈里斯堡大学教授兼合著者纳撒尼尔J.S阿什比(Nathaniel J.S. Ashby)说:“通过无偏见地自动识别潜在威胁，我们的目标是为预防犯罪、执法和军事应用提供较少受隐性偏见和情绪反应影响的工具。”</p>
<p class="translated"><a href="https://web.archive.org/web/20230331073348/https://web.archive.org/web/20200506013352/https://harrisburgu.edu/hu-facial-recognition-software-identifies-potential-criminals/">研究的其他作者</a>包括哈里斯堡大学助理教授Roozbeh Sadeghian和Jonathan W. Korn，一名博士生，在新闻稿中被强调为NYPD老兵。科恩称赞能够预测犯罪的软件是“执法机构的一大优势”</p>
<p class="translated">在反对该研究发表的公开信中，人工智能专家对该研究表示“严重关切”，并敦促斯普林格的审查委员会撤回其提议。这封信还呼吁其他出版商拒绝发表类似的未来研究，并列举了一系列理由，说明为什么面部识别和犯罪预测技术都应该非常谨慎，而不是针对已经脆弱的社区。</p>

<p class="translated">该出版物的反对者不仅担心研究人员打开了一个道德难题——他们还对研究本身提出了质疑，批评“不健全的科学前提、研究和方法，这些都是多年来跨越我们各自学科的无数研究揭露的。”</p><p class="piano-inline-promo"/>
<p class="translated"><em> <strong>更新:</strong>施普林格自然传播经理Felicitas Behrendt联系了TechCrunch，并提供了以下声明:<strong> <br/> </strong> </em></p>
<blockquote><p class="translated"><em>“我们承认对这篇论文的关注，并想澄清这篇论文从未被接受出版。它被提交给了一个即将召开的会议，斯普林格将在系列丛书《计算科学和计算智能汇刊》中发表会议记录，并通过了彻底的同行评审过程。系列编辑拒绝最终论文的决定是在6月16日星期二做出的，并在6月22日星期一正式传达给了作者。审查过程的细节和得出的结论在编辑、同行审查者和作者之间保密。”</em></p></blockquote>
<p class="translated">面部识别算法已经<a href="https://web.archive.org/web/20230331073348/https://www.technologyreview.com/2019/12/20/79/ai-face-recognition-racist-us-government-nist-study/">很久</a> <a href="https://web.archive.org/web/20230331073348/https://www.brennancenter.org/our-work/research-reports/predictive-policing-explained">被</a> <a href="https://web.archive.org/web/20230331073348/http://news.mit.edu/2018/study-finds-gender-skin-type-bias-artificial-intelligence-systems-0212">批评</a>在识别非白人面孔方面表现不佳，还有许多其他关于这类软件的科学和伦理问题。鉴于这项研究开发了面部识别软件，可用于预测性警务目的，这项技术的风险不能再高了。</p>
<p class="translated">“机器学习程序不是中立的；这封信的作者<a href="https://web.archive.org/web/20230331073348/https://medium.com/@CoalitionForCriticalTechnology/abolish-the-techtoprisonpipeline-9b5b14366b16">警告说</a>“研究议程和他们处理的数据集通常继承了关于世界的主流文化信仰。</p>
<p class="translated">“不加批判地接受默认假设，不可避免地会导致算法系统中的歧视性设计，复制将社会等级制度正常化并使针对边缘化群体的暴力合法化的思想。”</p>

			</div>

			</div>    
</body>
</html>