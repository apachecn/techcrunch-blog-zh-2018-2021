<html>
<head>
<title>Facebook tries to clean up Groups with new policies • TechCrunch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">脸书试图用新政策清理群体TechCrunch</h1>
<blockquote>原文：<a href="https://web.archive.org/web/https://techcrunch.com/2020/09/17/facebook-tries-to-clean-up-groups-with-new-policies/">https://web.archive.org/web/https://techcrunch.com/2020/09/17/facebook-tries-to-clean-up-groups-with-new-policies/</a></blockquote><div><div class="article-content">
				<p id="speakable-summary" class="translated">脸书今天早上宣布了一系列新规定，旨在进一步惩罚那些违反社区标准的人，特别是脸书团体。它还引入了旨在打击通过这些更私人的网络传播错误信息的规则。这些变化将影响那些曾帮助领导后来被取缔的组织的人以及参与这些组织的成员。这些规定还将从脸书的团体推荐中删除一些更具潜在危害的团体。</p>
<p class="translated">脸书<a href="https://web.archive.org/web/20230126005903/https://about.fb.com/news/2019/01/making-pages-more-transparent/">现有的累犯政策</a>旨在防止人们创建类似于那些因违反社区标准而被禁止的新团体。然而，该规则仅适用于组管理员。现在，脸书说<strong>管理员和版主</strong>在他们的群因违反政策被禁止后的“一段时间内”都不能创建任何新的群。脸书告诉我们这段时间是30天。如果30天后，管理员或版主试图创建另一个违规组，他们将再次暂停30天。</p>
<p class="translated">此外，在一个组中有任何违反社区标准行为的组<strong>成员</strong>现在需要在接下来的30天内获得帖子批准。这意味着他们的所有帖子都必须经过群组管理员或版主的预先批准。这可以帮助小组处理那些行为经常被标记的人，但也可能使有大量用户的小组不堪重负。脸书说，如果管理员或版主批准了违反社区标准的帖子，该群将被删除。</p>
<p class="translated">脸书也将要求群组有一个活动管理员。通常，管理员会变得很忙，辞职或离开他们的小组。脸书现在将尝试确定没有管理员参与的组，并主动向可能感兴趣的成员建议管理员角色。您可能已经从您的一些小组收到通知，需要一名管理员。如果是这样，那是因为脸书认为你有潜力领导这个团队，因为你没有违规历史。</p>
<p class="translated">它说，该公司将在未来几周内开始在没有活跃管理员的情况下对群组进行归档。当管理员离开并且没有其他人承担管理员角色时，就会发生这种情况。</p>
<p class="translated">这一变化可能有助于打击跨群体的无节制信息流动，这种流动可能导致垃圾邮件和错误信息迅速传播。正如Reddit等其他论坛网站所显示的那样，直接适度是有帮助的，但这通常是不够的。团队文化也会鼓励某些类型的内容，包括违反脸书规则的内容，而管理员通常愿意参与其中。</p>
<p class="translated">另一个变化将影响向用户建议哪些组。</p><p class="piano-inline-promo"/>
<p class="translated">该公司表示，卫生三基将不再被推荐，因为“人们从权威渠道获取健康信息至关重要”。</p>
<p class="translated">不幸的是，这一变化本身只能减轻误导健康信息的危险，但实际上并不能阻止它。因为仍然可以通过搜索找到健康群体，用户将能够容易地找到符合他们信念的群体，即使这些信念对他们自己或他人有害。</p>
<p class="translated">今天，有大量的团体继续传播误导性的健康信息，或推动用户尝试替代或未经测试的治疗方法。至少在脸书看来，这些小组参与者可能有“权利”在网上进行这些讨论，但对于这些小组是否应该被允许与更多专家主导的资源一样的搜索计费和可发现性，存在分歧。</p>
<p class="translated">例如，如果你今天在脸书搜索疫苗，脸书会很乐意给你指出几个告诉你不要接种的大群体。通过这样做，脸书已经有效地剥夺了医学专家和医生在健康相关问题上的权威，并把它交给了公众。乘以脸书数十亿用户的规模和所有主题，很容易明白为什么简单地不“推荐”一些群体几乎没有影响。</p>
<p class="translated">脸书今天也提醒用户注意其规则，以减少与暴力有关的T2组织的传播。它已经将它们从推荐中移除，限制它们的搜索，并表示在不久的将来，将减少它们在新闻提要中的内容。如果这些群体使用隐晦的语言和符号试图避免被标记，也会被删除。脸书说，最近，<a href="https://web.archive.org/web/20230126005903/https://about.fb.com/news/2020/08/addressing-movements-and-organizations-tied-to-violence/"> 790个与QAnon </a>有关的组织根据这项政策被清除。</p>
<p class="translated">然而，这种变化来得太少、太迟。QAnon，<a href="https://web.archive.org/web/20230126005903/https://www.bbc.com/news/53498434">多年未受检查</a>，已经<a href="https://web.archive.org/web/20230126005903/https://www.nytimes.com/2020/08/12/technology/qanon-save-the-children-trafficking.html">进入主流意识</a>，现在正在涉及那些可能甚至没有意识到<a href="https://web.archive.org/web/20230126005903/https://www.tampabay.com/news/nation-world/2020/08/14/whats-behind-viral-savethechildren-posts-look-to-qanon/">他们正被QAnon驱动的倡议所操纵</a>的人们。</p>
<p class="translated">此外，还有一个不小的问题，即脸书方面是否能切实执行自己提出的规则。快速浏览一下脸书对QAnon的搜索结果，你会发现答案是否定的。它可能已经删除了其中的790个群组，但在滚动了几分钟后，我们甚至无法到达使用“QAnon”作为关键字搜索后的群组搜索结果的底部。他们也不是反卡农组织。</p>
<p class="translated">这表明，脸书在这一领域的大部分工作是表演性的，而不是有效的。一次性清除有害团体并不等同于将资源和人员投入到将这些危险的边缘运动、有暴力倾向的组织者或反医学科学的信徒推向社会边缘的任务中——这是他们在离线、无联系的时代曾经占据的位置。相反，今天的脸书为这些群体提供了和其他人一样的组织工具，只是随着时间的推移，限制了他们的分散。</p>
<p class="translated">例如，脸书对与暴力有关的团体的政策实际上自相矛盾。它声称要删除讨论暴力的群组，但是<a href="https://web.archive.org/web/20230126005903/https://about.fb.com/news/2020/08/addressing-movements-and-organizations-tied-to-violence/">同时包含了一些规则</a>关于在推荐中限制这些相同的群组，并且在搜索中降低它们的排名。这表明，就连脸书也明白自己无法及时清除这些组织。</p>
<p class="translated">人们不同意脸书的角色是否应该包括调节其平台上的此类内容，或者在多大程度上这些内容应该作为“自由言论”受到保护但脸书从未真正在这里表明道德立场，或争辩说它不是一个政府机构，所以它可以根据自己的主张制定自己的规则。相反，它建立了大规模的互联网基础设施，而内容审核是事后的想法，是外包给不那么幸运的人的工作。现在，脸书甚至在成功解决自己制造的问题之前，就想为自己的清理工作赢得赞誉。</p>
			</div>

			</div>    
</body>
</html>