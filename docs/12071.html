<html>
<head>
<title>iPhones can now automatically recognize and label buttons and UI features for blind users • TechCrunch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">iPhones现在可以为盲人用户自动识别和标记按钮和用户界面功能TechCrunch</h1>
<blockquote>原文：<a href="https://web.archive.org/web/https://techcrunch.com/2020/12/03/iphones-can-now-automatically-recognize-and-label-buttons-and-ui-features-for-blind-users/">https://web.archive.org/web/https://techcrunch.com/2020/12/03/iphones-can-now-automatically-recognize-and-label-buttons-and-ui-features-for-blind-users/</a></blockquote><div><div class="article-content">
				<p id="speakable-summary" class="translated">苹果公司一直在努力为残疾用户构建功能，iOS上的VoiceOver对于任何有视力障碍的人来说都是一个非常宝贵的工具——假设界面的每个元素都被手动标记了。但该公司刚刚推出了一个全新的功能，使用机器学习来自动识别和标记每个按钮、滑块和标签。</p>
<p class="translated">现在在iOS 14中可以使用的屏幕识别是一个计算机视觉系统，它已经在数以千计的正在使用的应用程序图像上进行训练，学习按钮看起来像什么，图标意味着什么等等。这种系统非常灵活——根据你给它们的数据，它们可以成为识别猫、面部表情或用户界面不同部分的专家。</p>
<p class="translated">结果是，现在在任何应用程序中，用户都可以调用该功能，几分之一秒后，屏幕上的每个项目都将被标记。所谓“每一个”，他们指的是每一个——毕竟，屏幕阅读器需要知道视力正常的用户会看到并能够与之交互的每一件事情，从图像(iOS已经能够用一句话概括这些图像一段时间了)到常见的图标(主页、后退)和上下文相关的图标，比如随处可见的“…”菜单。</p>
<p class="translated">这个想法不是让手动标记过时——开发者最知道如何标记他们自己的应用程序，但是更新、变化的标准和挑战性的情况(例如，游戏中的界面)可能会导致事情不尽如人意。</p>

<p class="translated">我与苹果iOS辅助功能工程团队的Chris Fleizach和AI/ML辅助功能团队的Jeff Bigham聊了聊这个非常有用的新功能的起源。(这在一篇将于明年发表的论文中有所描述。)</p>
<p/><div id="attachment_2082611" class="wp-caption alignright"><a href="https://web.archive.org/web/20221208132318/https://techcrunch.com/wp-content/uploads/2020/12/apple-voiceover.jpg"><img aria-describedby="caption-attachment-2082611" decoding="async" loading="lazy" class="wp-image-2082611" src="../Images/920bfb9b4d33b6c35ac5d2400a19056b.png" alt="A phone showing a photo of two women smiling and voiceover describing the photo" srcset="https://web.archive.org/web/20221208132318im_/https://techcrunch.com/wp-content/uploads/2020/12/apple-voiceover.jpg 656w, https://web.archive.org/web/20221208132318im_/https://techcrunch.com/wp-content/uploads/2020/12/apple-voiceover.jpg?resize=150,150 150w, https://web.archive.org/web/20221208132318im_/https://techcrunch.com/wp-content/uploads/2020/12/apple-voiceover.jpg?resize=300,298 300w, https://web.archive.org/web/20221208132318im_/https://techcrunch.com/wp-content/uploads/2020/12/apple-voiceover.jpg?resize=32,32 32w, https://web.archive.org/web/20221208132318im_/https://techcrunch.com/wp-content/uploads/2020/12/apple-voiceover.jpg?resize=50,50 50w, https://web.archive.org/web/20221208132318im_/https://techcrunch.com/wp-content/uploads/2020/12/apple-voiceover.jpg?resize=64,64 64w, https://web.archive.org/web/20221208132318im_/https://techcrunch.com/wp-content/uploads/2020/12/apple-voiceover.jpg?resize=96,96 96w, https://web.archive.org/web/20221208132318im_/https://techcrunch.com/wp-content/uploads/2020/12/apple-voiceover.jpg?resize=128,128 128w" sizes="(max-width: 328px) 100vw, 328px" data-original-src="https://web.archive.org/web/20221208132318im_/https://techcrunch.com/wp-content/uploads/2020/12/apple-voiceover.jpg"/></a><p id="caption-attachment-2082611" class="wp-caption-text translated">图片来源:苹果公司</p></div><p class="piano-inline-promo"/>
<p class="translated">“我们寻找可以在可访问性方面取得进展的领域，比如图像描述，”Fleizach说。“在iOS 13中，我们自动标记图标——屏幕识别又向前迈进了一步。我们可以查看屏幕上的像素，并确定您可以与之交互的对象的层次，所有这些都在十分之一秒内发生在设备上。”</p>
<p class="translated">确切地说，这个想法并不新鲜；Bigham提到了一个屏幕阅读器，直言不讳，它在几年前就试图使用像素级数据来识别UI元素。尽管该系统需要精确匹配，但机器学习系统的模糊逻辑和iPhones内置人工智能加速器的速度意味着屏幕识别更加灵活和强大。</p>
<p class="translated">就在几年前，这是不可能的——机器学习的状态和缺乏执行它的专用单元意味着这样的事情对系统来说是极其繁重的，需要更长的时间，并且可能一直在耗尽电池。</p>
<p class="translated">但是一旦这种系统看起来可行，团队就在他们专门的可访问性人员和测试社区的帮助下开始原型化工作。</p>
<p class="translated">“长期以来，VoiceOver一直是视觉辅助功能的旗手。如果你看看屏幕识别的开发步骤，它是基于跨团队的合作——贯穿始终的可访问性，我们在数据收集和注释、AI/ML方面的合作伙伴，当然还有设计。我们这样做是为了确保我们的机器学习开发继续朝着优秀的用户体验推进，”Bigham说。</p>
<p class="embed breakout embed--video embed--youtube translated"><iframe title="How to navigate your iPhone or iPad with VoiceOver — Apple Support" src="https://web.archive.org/web/20221208132318if_/https://www.youtube.com/embed/qDm7GiKra28?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="">视频</iframe></p>
<p class="translated">这是通过拍摄成千上万个流行应用和游戏的截图，然后手动将它们标记为几个标准UI元素之一来完成的。这些经过标记的数据被输入到机器学习系统中，该系统很快就能熟练地自行挑选出这些相同的元素。</p>
<p class="translated">这并不像听起来那么简单——作为人类，我们已经非常善于理解特定图形或文本的意图，所以我们经常可以浏览甚至抽象或创造性设计的界面。对于机器学习模型来说，它远没有那么清晰，团队不得不与它合作，创建一套复杂的规则和层次结构，以确保最终的屏幕阅读器解释是有意义的。</p>
<p class="translated">这项新功能应该有助于让数百万有视觉障碍的用户更容易使用或根本无法使用应用程序。您可以通过前往“辅助功能设置”，然后VoiceOver，然后VoiceOver识别来打开它，在这里您可以打开和关闭图像、屏幕和文本识别。</p>
<p class="translated">将屏幕识别带到其他平台，如Mac，并不是一件小事，所以现在还不要抱太大希望。但是这个原理是合理的，尽管这个模型本身并不能推广到桌面应用，桌面应用与移动应用有很大的不同。也许其他人会承担这项任务；人工智能驱动的辅助功能的前景才刚刚开始实现。<span data-mce-type="bookmark" class="mce_SELRES_start">T3】</span></p>
<p class="translated">TechCrunch主编Matthew Panzarino最近与苹果公司的Chris Fleizach(iOS可访问性工程主管)和Sarah Herrlinger(全球可访问性政策和倡议高级主管)进行了交谈——请点击此处查看采访:</p>
<p class="embed breakout embed--video embed--youtube translated"><iframe title="Designing for everyone: Accessibility innovation at Apple" src="https://web.archive.org/web/20221208132318if_/https://www.youtube.com/embed/v47mD60ertI?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="">视频</iframe></p>
			</div>

			</div>    
</body>
</html>