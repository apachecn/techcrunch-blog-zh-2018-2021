<html>
<head>
<title>EU plan for risk-based AI rules to set fines as high as 4% of global turnover, per leaked draft </title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">根据泄露的草案，欧盟计划基于风险的人工智能规则将罚款设定为全球营业额的 4%</h1>
<blockquote>原文：<a href="https://web.archive.org/web/https://techcrunch.com/2021/04/14/eu-plan-for-risk-based-ai-rules-to-set-fines-as-high-as-4-of-global-turnover-per-leaked-draft/">https://web.archive.org/web/https://techcrunch.com/2021/04/14/eu-plan-for-risk-based-ai-rules-to-set-fines-as-high-as-4-of-global-turnover-per-leaked-draft/</a></blockquote><div><div class="article-content">
				<p id="speakable-summary" class="translated">根据一份<a href="https://web.archive.org/web/20230326173101/https://drive.google.com/file/d/1ZaBPsfor_aHKNeeyXxk9uJfTru747EOn/view">泄露的人工智能法规草案</a>—<a href="https://web.archive.org/web/20230326173101/https://pro.politico.eu/news/134201">政治</a>早些时候报道的——预计将于下周正式公布，正在起草人工智能应用规则的欧盟立法者正在考虑对一系列被禁止的用例处以高达全球年营业额 4%的罚款(或€2000 万，如果更多的话)。</p>
<p class="translated">监管人工智能的计划已经酝酿了一段时间。回到【2020 年 2 月欧洲委员会发布了一份白皮书，概述了监管所谓的人工智能“高风险”应用的计划。</p>
<p class="translated">当时，欧盟立法者正在玩弄部门焦点——将能源和招聘等特定部门视为风险载体。然而，根据泄露的草案，这种方法似乎已经被重新考虑过——它没有将人工智能风险的讨论限制在特定的行业或部门。</p>
<p class="translated">相反，重点是高风险人工智能应用的合规性要求，无论它们可能出现在哪里(然而，武器/军事用途被明确排除在外，因为此类使用案例不在欧盟条约范围内)。尽管从这份草案中还不太清楚“高风险”将如何定义。</p>
<p class="translated">欧盟委员会的首要目标是通过一个充满“欧盟价值观”的合规检查和平衡系统来提高公众对人工智能的信任，以鼓励人们接受所谓的“值得信赖”和“以人为中心”的人工智能。因此，即使是不被视为“高风险”的人工智能应用程序的制造商，也仍将被鼓励采用行为准则——正如该委员会所说，“以促进适用于高风险人工智能系统的强制性要求的自愿应用”。</p>
<p class="translated">该法规的另一部分涉及支持欧盟人工智能发展的措施——推动成员国建立监管沙盒计划，在将人工智能系统推向市场之前，可以禁止初创公司和中小企业获得开发和测试人工智能系统的支持。</p>
<p class="translated">草案指出，主管部门“应有权对参与沙盒的实体的人工智能项目行使自由裁量权和比例杠杆，同时充分保留当局的监督权和纠正权”。</p><p class="piano-inline-promo"/>
<h2 class="translated">什么是高风险 AI？</h2>
<p class="translated">根据计划中的规则，那些打算应用人工智能的人将需要确定某个特定用例是否是“高风险”的，从而确定他们是否需要进行强制性的上市前合规性评估。</p>
<p class="translated">“人工智能系统的高风险分类应基于其预期目的——这应指人工智能系统的预期用途，包括使用的具体背景和条件——并通过考虑它是否可能造成某些伤害，以及如果是的话，可能伤害的严重性和发生的可能性，分两步确定，”草案中的一段陈述说。</p>
<p class="translated">“就本法规而言，将人工智能系统分类为高风险并不一定意味着根据行业法规的标准，该系统本身或整个产品一定会被视为‘高风险’，”该文本还规定。</p>
<p class="translated">草案中列出了与高风险人工智能系统相关的“伤害”示例，包括:“人身伤害或死亡、财产损失、对整个社会的系统性不利影响、为关键经济和社会活动的正常进行提供基本服务的重大中断、对个人的金融、教育或职业机会的不利影响、对获得公共服务和任何形式的公共援助的不利影响，以及对[欧洲]基本权利的不利影响。”</p>
<p class="translated">还讨论了几个高风险应用的例子，包括招聘系统；提供教育或职业培训机构的系统；紧急服务调度系统；信誉评估；涉及确定纳税人出资的福利分配的系统；适用于预防、侦查和起诉犯罪的决策系统；以及用于协助法官的决策系统。</p>
<p class="translated">根据立法计划，只要符合合规要求——如建立风险管理体系和进行上市后监督，包括通过质量管理体系——此类体系就不会被禁止进入欧盟市场。</p>
<p class="translated">其他要求包括安全领域，以及人工智能实现性能准确性的一致性——规定在意识到这一点后不迟于 15 天向监督机构报告“构成违反义务的任何严重事件或人工智能系统的任何故障”。</p>
<p class="translated">“高风险人工智能系统可能会被投放到欧盟市场，或者在符合强制性要求的情况下投入服务，”该文本指出。</p>
<p class="translated">“考虑到人工智能系统的预期目的，并根据提供商建立的风险管理系统，应遵守关于在欧盟市场上投放或投入服务的高风险人工智能系统的强制性要求。</p>
<p class="translated">“除其他事项外，供应商确定的风险控制管理措施应基于对强制性要求组合应用产生的影响和可能的相互作用的适当考虑，并考虑公认的技术水平，也包括相关协调标准或共同规范中反映的技术水平。”</p>

<h2 class="translated">禁止的做法和生物识别技术</h2>
<p class="translated">根据这份泄露的草案，某些人工智能“做法”被列为计划中的法律第 4 条所禁止的——包括可能导致歧视的大规模监控系统和通用社会评分系统的(商业)应用。</p>
<p class="translated">旨在操纵人类行为、决策或意见以达到有害目的的人工智能系统(如通过黑暗模式设计 ui)也被列为第 4 条禁止的系统；使用个人数据生成预测以便(有害地)针对个人或人群的脆弱性的系统也是如此。</p>
<p class="translated">一个不经意的读者可能会认为，该法规是在提议一举禁止基于人群追踪的行为广告等做法——也就是脸书和谷歌等公司的商业模式。然而，这是假设广告技术巨头会接受他们的工具对用户有不利影响。</p>
<p class="translated">相反，他们的监管规避策略是基于宣称两极对立；因此，脸书谈到了“相关”广告。因此，该文本(如所写的)看起来将成为(然而)更旷日持久的法律斗争的一个处方，试图使欧盟法律坚持反对科技巨头的自利解释。</p>
<p class="translated">该草案的早期陈述总结了被禁止做法的理由，其中指出:“应该承认，人工智能可以实现新的操纵、成瘾、社会控制和不分青红皂白的监控做法，这些做法特别有害，应该被禁止，因为违反了尊重人类尊严、自由、民主、法治和尊重人权的欧盟价值观。”</p>
<p class="translated">值得注意的是，该委员会一直避免提议禁止在公共场所使用面部识别——根据去年年初泄露的草案<a href="https://web.archive.org/web/20230326173101/https://techcrunch.com/2020/01/17/eu-lawmakers-are-eyeing-risk-based-rules-for-ai-per-leaked-white-paper/"/>，在去年的白皮书避开禁令之前，它显然一直在考虑。</p>
<p class="translated">在泄露的草案中，公共场所的“远程生物识别”被挑选出来进行“通过通知机构的参与进行更严格的合规性评估程序”——也称为“解决技术使用所隐含的特定风险的授权程序”，包括强制性的数据保护影响评估——而不是高风险人工智能的大多数其他应用(允许通过自我评估满足要求)。</p>
<p class="translated">“此外，授权机构应在其评估中考虑用于特定目的的系统的不准确性造成伤害的可能性和严重性，特别是在年龄、种族、性别或残疾方面，”草案说。“应进一步考虑社会影响，特别是考虑民主和公民参与，以及将个人纳入参考数据库的方法、必要性和相称性。”</p>
<p class="translated">作为合规流程的一部分,“可能主要导致对人身安全的不利影响”的人工智能系统也需要接受更高的监管要求。</p>
<p class="translated">设想中的所有高风险人工智能的符合性评估系统正在进行中，草案指出:“每当发生可能影响系统符合本法规的变化或当系统的预期目的发生变化时，人工智能系统进行新的符合性评估是适当的。”</p>
<p class="translated">“对于在投放市场或投入服务后继续‘学习’的人工智能系统(即，它们自动适应如何执行功能)，在一致性评估时尚未预先确定和评估的算法和性能的变化将导致人工智能系统的新一致性<br/>评估，”它补充道。</p>
<p class="translated">合规企业的胡萝卜是展示一个“CE”标志，以帮助他们赢得用户的信任，并顺利进入欧盟单一市场。</p>
<p class="translated">“高风险人工智能系统应该带有 CE 标志，以表明它们符合本法规的要求，以便它们可以在欧盟内自由移动，”该文本指出，并补充说:“成员国不应该对符合本法规要求的人工智能系统投入市场或投入服务设置障碍。”</p>

<h2 class="translated">机器人和 deepfakes 的透明度</h2>
<p class="translated">除了寻求取缔一些做法，并建立一个泛欧盟规则系统，以将“高风险”人工智能系统安全地推向市场——供应商预计将进行(主要是自我)评估并履行合规义务(如围绕用于训练模型的数据集的质量；记录保存/文件编制；人为监督；透明度；准确性),并进行持续的上市后监控——拟议的法规旨在降低人工智能被用于欺骗人们的风险。</p>
<p class="translated">它通过建议旨在与自然人交互的人工智能系统(也称为语音人工智能/聊天机器人等)的“协调透明规则”来实现这一点；以及用于生成或操纵图像、音频或视频内容的 AI 系统(又名 deepfakes)。</p>
<p class="translated">“某些旨在与自然人互动或生成内容的人工智能系统可能会带来特定的假冒或欺骗风险，无论它们是否符合高风险标准。在某些情况下，这些系统的使用因此应该遵守特定的透明度义务，而不损害高风险人工智能系统的要求和义务，”该文本称。</p>
<p class="translated">“特别是，自然人应该被通知他们正在与人工智能系统进行交互，除非从环境和使用上下文来看这是显而易见的。此外，使用人工智能系统生成或操纵图像、音频或视频内容的用户，如果这些内容与现有的人、地点或事件非常相似，并且在合理的人看来是真实的，则应通过相应地标记人工智能输出并披露其人工来源来披露该内容是人工创建或操纵的。</p>
<p class="translated">“如果为了保障公共安全或为了行使某人的合法权利或自由，如讽刺、戏仿或艺术和科学自由，而有必要使用此类内容，则不应适用这一标签义务，并须遵守对第三方权利和自由的适当保障。”</p>

<h2 class="translated">执法呢？</h2>
<p class="translated">尽管拟议的人工智能制度尚未由欧盟委员会正式公布——因此下周之前细节仍可能发生变化——一个主要的问号笼罩着围绕(往往很复杂的)人工智能的具体应用的全新合规层如何能够得到有效监督，以及任何违规行为如何得到执行，特别是考虑到欧盟数据保护制度(该制度从 2018 年开始实施)的执行工作一直存在弱点。</p>
<p class="translated">因此，虽然要求高风险人工智能的提供商负责将他们的系统投入市场(并因此遵守所有各种规定，其中也包括在委员会打算维护的欧盟数据库中注册高风险人工智能系统)，但该提案将执法权留给了成员国——它们将负责指定一个或多个国家主管当局来监督监督监督制度的应用。</p>
<p class="translated">我们已经看到了这个故事是如何与一般数据保护法规发生冲突的。欧盟委员会自己也承认 GDPR 的执行并没有在整个欧盟得到一致或有力的应用——所以一个主要的问题是这些羽翼未丰的人工智能规则将如何避免同样的选择法庭的命运——T2？</p>

<p class="translated">“成员国应采取一切必要措施，确保本法规的规定得到执行，包括对违反规定的行为规定有效、适度和劝阻性的处罚。对于某些特定的违规行为，成员国应该考虑本法规中规定的界限和标准，”草案写道。</p>
<p class="translated">欧盟委员会确实增加了一条警告——如果成员国执法不力，欧盟委员会可能会介入。但短期内不会有不同的执行方式，这表明同样的老问题可能会出现。</p>
<p class="translated">“由于该法规的目标，即创造条件建立一个信任的生态系统，将人工智能投入市场，投入服务和在欧盟内使用，不能由成员国充分实现，而是由于行动的规模或影响，可以在欧盟层面更好地实现，欧盟可以根据《欧洲联盟条约》第 5 条规定的辅助性原则采取措施，”是委员会未来执法失败的后备措施。</p>
<p class="translated">人工智能的监督计划包括建立一个类似于 GDPR 欧洲数据保护委员会(European Data Protection Board)的镜像实体——将被称为欧洲人工智能委员会(European Artificial Intelligence Board)——该委员会将通过为欧盟立法者发布相关建议和意见来支持该法规的应用，例如围绕被禁止的人工智能实践和高风险系统的列表。</p>


<p> </p>
			</div>

			</div>    
</body>
</html>