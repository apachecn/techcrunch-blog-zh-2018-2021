<html>
<head>
<title>YouTube's recommender AI still a horror show, finds major crowdsourced study • TechCrunch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">YouTube 的推荐者 AI 仍然是一个恐怖节目，发现主要的众包研究 TechCrunch</h1>
<blockquote>原文：<a href="https://web.archive.org/web/https://techcrunch.com/2021/07/07/youtubes-recommender-ai-still-a-horrorshow-finds-major-crowdsourced-study/">https://web.archive.org/web/https://techcrunch.com/2021/07/07/youtubes-recommender-ai-still-a-horrorshow-finds-major-crowdsourced-study/</a></blockquote><div><div class="article-content">
				<p id="speakable-summary" class="translated">多年来，YouTube 的视频推荐算法<a href="https://web.archive.org/web/20230205132633/https://www.theguardian.com/technology/2018/feb/02/how-youtubes-algorithm-distorts-truth">一直被指责为</a>通过向用户提供人工智能放大的仇恨言论、政治极端主义和/或阴谋垃圾/虚假信息，试图让数十亿眼球停留在广告库存上，从而助长了一系列社会弊病。</p>
<p class="translated">虽然 YouTube 的科技巨头谷歌偶尔会对围绕该算法的反社会建议爆发的负面宣传做出回应——宣布一些<a href="https://web.archive.org/web/20230205132633/https://techcrunch.com/2017/11/14/in-major-policy-change-youtube-is-now-taking-down-more-videos-of-known-extremists/">政策调整</a>或限制/清除<a href="https://web.archive.org/web/20230205132633/https://techcrunch.com/2019/04/02/youtube-tightens-restrictions-on-channel-of-uk-far-right-activist-but-no-ban/">奇怪的仇恨账户</a>——但尚不清楚该平台推广可怕的不健康点击诱饵的倾向实际上已经重新启动了多远。</p>
<p class="translated">怀疑仍然远远不够。</p>
<p class="translated">Mozilla 今天发布的新研究支持了这一观点，表明 YouTube 的人工智能继续膨胀成堆的“底层”/低级/分裂/虚假内容——试图通过引发人们的愤怒感、缝合分裂/两极分化或传播毫无根据/有害的虚假信息来抓住眼球的内容——这反过来暗示 YouTube 在推荐糟糕内容方面的问题确实是系统性的；这是该平台获取浏览量以投放广告的贪婪胃口的副作用。</p>
<p class="translated">根据 Mozilla 的研究，YouTube 的人工智能仍然表现如此糟糕，这也表明谷歌在用表面的改革主张模糊批评方面相当成功。</p>
<p class="translated">它在这方面的成功很可能是主要的保护机制，通过方便的“商业秘密”的保护，使推荐引擎的算法工作(和相关数据)隐藏在公众视野和外部监督之外</p>
<p class="translated">但是，可能有助于破解专有人工智能黑匣子的法规现在正在酝酿之中——至少在欧洲是这样。</p><p class="piano-inline-promo"/>
<p class="translated">为了修复 YouTube 的算法，Mozilla 呼吁“常识性的透明法律、更好的监督和消费者压力”——建议将法律结合起来，要求人工智能系统透明；保护独立研究人员，使他们能够质疑算法的影响；让平台用户拥有强大的控制能力(比如选择退出“个性化”推荐的能力)是遏制 YouTube 人工智能最糟糕的过度行为所需要的。</p>

<h2 class="translated">遗憾的是，YouTube 用户有一些…</h2>
<p class="translated">为了收集向 YouTube 用户提出的具体建议的数据——谷歌通常不会向外部研究人员提供这些信息——Mozilla 采取了众包方法，通过浏览器扩展(称为<a href="https://web.archive.org/web/20230205132633/https://foundation.mozilla.org/en/campaigns/regrets-reporter/"> RegretsReporter </a>)让用户自我报告他们“后悔”观看的 YouTube 视频。</p>
<p class="translated">该工具可以生成一份报告，其中包括向用户推荐的视频的详细信息，以及之前的视频观看次数，以帮助构建 YouTube 推荐系统的运行情况。(或者，嗯，视情况而定，“功能障碍”。)</p>
<p class="translated">众包志愿者的数据为 Mozilla 的研究提供了数据，他们报告了各种各样的“遗憾”，包括传播新冠肺炎恐惧的视频、政治错误信息和“非常不合适”的儿童漫画，报告称，最常报告的内容类别是错误信息、暴力/图形内容、仇恨言论和垃圾邮件/诈骗。</p>
<p class="translated">相当大一部分(71%)的后悔报告来自 YouTube 算法本身推荐的视频，突显了人工智能在将垃圾内容推入人们眼球中的重要作用。</p>
<p class="translated">研究还发现，推荐视频被志愿者举报的可能性比他们自己搜索的视频高出 40%。</p>
<p class="translated">Mozilla 甚至发现了“几个”例子，推荐算法将违反 YouTube 自己的社区准则和/或与之前观看的视频无关的内容放在用户面前。所以很明显失败了。</p>
<p class="translated">一个非常值得注意的发现是，令人遗憾的内容对非英语国家的 YouTube 用户来说似乎是一个更大的问题:Mozilla 发现，在英语不是主要语言的国家，YouTube 后悔率要高 60%——报告称，巴西、德国和法国的后悔率“特别高”。(这三个市场都不能被归类为次要的国际市场。)</p>
<p class="translated">根据该报告，与疫情相关的遗憾在非英语国家也特别普遍——在全球健康危机持续的情况下，这是一个令人担忧的细节。</p>
<p class="translated">这项众包研究 Mozilla 称之为 YouTube 推荐算法中有史以来最大的研究——利用了超过 37000 名安装了该扩展的 YouTube 用户的数据，尽管它是来自 91 个国家的 1162 名<span>志愿者的子集，他们提交的报告标记了 3362 个令人遗憾的视频，这些视频是报告直接利用的。</span></p>
<p class="translated">这些报告是在 2020 年 7 月至 2021 年 5 月之间生成的。</p>
<p class="translated">Mozilla 说的一个 YouTube“后悔”到底是什么意思？它说这是一个基于用户在 YouTube 上自我报告糟糕经历的众包概念，所以这是一个主观的衡量标准。但 Mozilla 认为，采取这种“以人为本”的方法集中了互联网用户的生活体验，因此有助于突出边缘化和/或弱势人群和社区的体验(例如，与只应用“伤害”的狭义法律定义相比)。</p>
<p class="translated">Mozilla 的高级宣传经理兼该项目的首席研究员 Brandi Geurkink 在讨论研究目的时解释道:“我们想进一步询问和探索[人们掉进 YouTube“兔子洞”的经历]，并坦率地证实其中的一些故事——但也想进一步了解其中出现的一些趋势。”。</p>
<p class="translated">“我在做这项工作时的主要感受是——我猜——震惊于一些我们预期的情况得到了证实……就参与人数和我们使用的方法而言，这仍然是一项有限的研究，但即使如此——它也非常简单；数据只是显示我们的一些想法被证实了。</p>
<p class="translated">“像算法推荐内容这样的事情本质上是偶然的，后来就像‘哎呀，这实际上违反了我们的政策；我们不应该主动向人们暗示这一点……以及非英语用户群体验更差之类的事情——这些是你听到过很多坊间讨论的事情，活动人士已经提出了这些问题。但我只是想——哦，哇，它实际上在我们的数据中显示得非常清楚。”</p>
<p class="translated">Mozilla 表示，众包研究发现了“大量例子”，这些报道内容可能或实际上违反了 YouTube 的社区准则，例如仇恨言论或被揭穿的政治和科学错误信息。</p>
<p class="translated">但它也表示，这些报告标记了许多 YouTube“可能”认为是“边缘内容”的内容。又名，更难分类的东西——垃圾/低质量的视频，可能符合可接受的标准，因此平台的算法审核系统可能更难回应(因此内容也可能在被关闭的风险下存活更长时间)。</p>
<p class="translated">然而，该报告指出的一个相关问题是，YouTube 没有提供边界内容的定义——尽管在其自己的指南中讨论了这一类别——因此，Mozilla 表示，这使得研究人员的假设，即志愿者报告的大部分“遗憾”内容很可能属于 YouTube 自己的“边界内容”类别，这无法核实。</p>
<p class="translated">独立研究谷歌技术和流程的社会影响的挑战是这项研究的一个持续主题。但 Mozilla 的报告也指责这家科技巨头以“惰性和不透明”应对 YouTube 的批评。</p>
<p class="translated">那里也不是一个人。批评者长期以来一直指责 YouTube 的广告巨头母公司从仇恨愤怒和有害虚假信息产生的参与中获利——允许“<a href="https://web.archive.org/web/20230205132633/https://techcrunch.com/2017/12/19/youtube-more-ai-can-fix-ai-generated-bubbles-of-hate/">人工智能产生的仇恨气泡</a>”浮出水面越来越有害(因此也越来越吸引人)，让不知情的 YouTube 用户接触到越来越令人不快和极端的观点，即使谷歌开始在用户生成内容的保护伞下保护其低级内容业务。</p>
<p class="translated">事实上，“掉进 YouTube 的兔子洞”已经成为一个广为人知的比喻，用来描述不知情的互联网用户被拖入网络最黑暗和最肮脏的角落的过程。这种用户重新编程发生在光天化日之下，通过人工智能生成的建议，对人们大喊大叫，让他们从主流网络平台内部跟踪阴谋面包屑。</p>
<p class="translated">早在 2017 年，当人们对网络恐怖主义和社交媒体上 ISIS 内容的扩散感到担忧时，欧洲的政治家们指责 YouTube 的算法正是这样:<a href="https://web.archive.org/web/20230205132633/https://techcrunch.com/2017/12/19/youtube-more-ai-can-fix-ai-generated-bubbles-of-hate/">自动化激进化</a>。</p>
<p class="translated">然而，仍然很难获得确凿的数据来支持个别 YouTube 用户在谷歌平台上观看数小时极端主义内容或阴谋论垃圾后被“激进化”的轶事报道。</p>
<p class="translated">前 YouTube 内部人士 Guillaume Chaslot 是一位著名的评论家，他试图通过他的<a href="https://web.archive.org/web/20230205132633/https://www.algotransparency.org/"> algotransparency </a>项目，揭开保护专有技术免受更深入审查的帷幕。</p>
<p class="translated">Mozilla 的众包研究通过整理用户自己的糟糕体验报告，勾勒出 YouTube 人工智能的一幅大致——也是大致存在问题的——的图景，从而加强了这些努力。</p>
<p class="translated">当然，从外部取样只有谷歌完全掌握的平台级数据(以其真正的深度和维度)不可能是全貌——特别是自我报告，可能会将自己的一套偏见引入 Mozilla 的数据集。但是有效地研究 Big Tech 的黑盒是这项研究的一个关键点，因为 Mozilla 提倡对平台能力进行适当的监督。</p>
<p class="translated">在一系列建议中，该报告呼吁“强大的透明度、审查，并让人们控制推荐算法”——认为如果没有对该平台的适当监督，YouTube 将继续有害，因为它会盲目地让人们接触有害和脑残的内容。</p>
<p class="translated">从报告的其他细节中可以看出，YouTube 的许多功能缺乏透明度，这是个问题。例如，Mozilla 发现，大约 9%的推荐后悔(或近 200 个视频)已经被删除——原因多种多样，并不总是很清楚(有时，可能是在内容被 YouTube 报道并判定违反其准则之后)。</p>
<p class="translated">总的来说，在因为某种原因被删除之前，仅这个视频子集就有 1.6 亿次观看。</p>
<p class="translated">在其他发现中，研究发现遗憾的观点往往在平台上表现良好。</p>
<p class="translated">一个特别明显的指标是，报告的遗憾比志愿者在该平台上观看的其他视频每天多获得了整整 70%的浏览量——这为 YouTube 的参与优化算法更多地选择触发/误导内容而不是高质量(有思想/有信息)的内容的论点提供了证据，因为它带来了点击量。</p>
<p class="translated">虽然这可能对谷歌的广告业务有好处，但对于重视真实信息而非废话的民主社会来说，这显然是一个负面消息；关于人造/放大二进制的真正公开辩论；和建设性的公民凝聚力。</p>
<p class="translated">但是，如果没有对广告平台的法律强制透明度要求——以及最有可能的是，以审计权力为特征的监管监督和执行——这些科技巨头将继续受到激励，以视而不见并以社会为代价获利。</p>
<p class="translated">Mozilla 的报告还强调了 YouTube 的算法明显由与内容本身无关的逻辑驱动的情况——研究发现，在研究人员拥有参与者在报告后悔之前观看的视频数据的情况下，43.6%的情况下，推荐与之前的视频完全无关。</p>
<p class="translated">该报告给出了一些违反逻辑的人工智能内容支点/跳跃/陷阱的例子——例如，一个人观看了关于美国军队的视频，然后被推荐了一个题为“男人在病毒视频中羞辱女权主义者”的厌恶女性的视频。</p>
<p class="translated">在另一个例子中，一个人看了一个关于软件权利的视频，然后被推荐了一个关于枪支权利的视频。所以两个权利使得另一个错误的 YouTube 推荐就在那里。</p>
<p class="translated">在第三个例子中，一个人观看了阿特·加芬克尔的音乐视频，然后被推荐了一个题为“特朗普辩论主持人被曝与民主党关系密切，媒体偏见达到临界点”的政治视频</p>
<p class="translated">对此唯一理智的反应是，嗯，什么？？？</p>
<p class="translated">在这种情况下，YouTube 的输出看起来——充其量——是某种“人工智能大脑放屁”</p>
<p class="translated">一个宽松的解释可能是算法被愚蠢地混淆了。尽管如此，在报告中引用的许多例子中，这种混乱正导致 YouTube 用户转向带有右倾政治偏见的内容。这看起来很奇怪。</p>
<p class="translated">当被问及她认为最令人担忧的发现是什么时，Mozilla 的 Geurkink 告诉 TechCrunch:“一个是错误信息如何明显成为平台上的主要问题。我认为，根据我们与 Mozilla 支持者和来自世界各地的人们的交谈，这是人们在网上关注的一件非常明显的事情。因此，看到这成为 YouTube 算法的最大问题，真的让我很担心。”</p>
<p class="translated">她还强调了建议对非英语用户更糟糕的问题，这是另一个主要问题，表明用户对平台影响体验的全球不平等“没有得到足够的重视”——即使这些问题得到了讨论。</p>
<p class="translated">谷歌发言人在一份声明中回应了 Mozilla 的报告，向我们发送了以下声明:</p>
<blockquote><p class="translated">我们的推荐系统的目标是将观众与他们喜欢的内容联系起来，在任何一天，仅在主页上就有超过 2 亿个视频被推荐。超过 800 亿条信息用于帮助我们的系统，包括观众对他们想看什么的调查反馈。我们不断努力改善 YouTube 上的体验，仅在过去一年，我们就推出了 30 多项不同的改变，以减少有害内容的推荐。由于这一变化，来自我们推荐的边缘内容的消费现在大大低于 1%。</p></blockquote>
<p class="translated">谷歌还声称，它欢迎对 YouTube 的研究，并暗示它正在探索引入外部研究人员研究该平台的选项，但没有提供这方面的任何具体信息。</p>
<p class="translated">与此同时，它的回应质疑 Mozilla 的研究如何定义“令人遗憾的”内容——并继续声称，它自己的用户调查通常显示用户对 YouTube 推荐的内容感到满意。</p>
<p class="translated">在进一步的不可引用的言论中，谷歌指出，今年早些时候，它开始披露 YouTube 的“<a href="https://web.archive.org/web/20230205132633/https://blog.youtube/inside-youtube/building-greater-transparency-and-accountability/">违规观看率”</a> (VVR)指标——首次披露 YouTube 上来自违反其政策的内容的观看百分比。</p>
<p class="translated">最近的 VVR 为 0.16%-0.18%——谷歌表示，这意味着 YouTube 上每 10，000 次观看中，有 16-18 次来自违规内容。该公司表示，与 2017 年同季度相比，这一数字下降了 70%以上-认为其在机器学习方面的投资是这一下降的主要原因。</p>
<p class="translated">然而，正如 Geurkink 所指出的那样，如果谷歌不发布更多数据来对其人工智能在加速查看其规则状态不应在其平台上查看的内容方面的参与程度进行语境化和量化，VVR 的用途是有限的。没有这些关键数据，人们肯定会怀疑 VVR 是一个不错的误导。</p>
<p class="translated">“比(VVR)更进一步的，也是真正非常有帮助的，是理解推荐算法在其中扮演的角色？”Geurkink 告诉我们这一点，并补充说:“这就是一个完整的黑盒蒸馏器。在缺乏更大透明度的情况下，不得不半信半疑地看待(谷歌的)进步声明。”</p>
<p class="translated">谷歌还标记了 2019 年对 YouTube 推荐算法处理“边缘内容”(即不违反政策但落入有问题的灰色区域的内容)的方式所做的改变，称这一调整还导致这类内容的观看时间下降了 70%。</p>
<p class="translated">尽管该公司证实这一临界类别是一场可移动的盛宴——称其考虑了变化的趋势和背景，并与专家合作确定什么被归类为临界——这使得上述百分比下降几乎没有意义，因为没有固定的基线来衡量。</p>
<p class="translated">值得注意的是，谷歌对 Mozilla 报告的回应没有提及非英语市场调查参与者报告的糟糕体验。Geurkink 认为，一般来说，YouTube 应用的许多所谓的缓解措施都是有地理限制的——也就是说，仅限于美国和英国等英语市场(或者至少先到达这些市场，然后再缓慢地推广到其他地方。)</p>
<p class="translated">例如，2019 年 1 月为减少美国阴谋论内容的放大而进行的调整在几个月后才扩展到英国市场——在<a href="https://web.archive.org/web/20230205132633/https://techcrunch.com/2019/08/28/youtube-to-reduce-conspiracy-theory-recommendations-in-the-uk/">8 月</a>。</p>
<p class="translated">“在过去的几年里，YouTube 一直在报告他们在美国和英语市场推荐有害或边缘内容的进展，”<span>她还</span>说。“很少有人质疑这一点——世界其他地方呢？对我来说，这确实值得更多的关注和审视。”</p>
<p class="translated">我们要求谷歌确认它是否已经在全球范围内应用了 2019 年阴谋论相关的变化——一位女发言人告诉我们，它已经应用了。但向 Mozilla 报告的“令人遗憾”的内容在非英语市场的比例要高得多，这仍然值得注意。</p>
<p class="translated">虽然可能有其他因素在起作用，这可能解释了一些不成比例的高报道，但这一发现也可能表明，就 YouTube 的负面影响而言，谷歌将最大的资源用于其声誉风险和机器学习技术自动内容分类能力最强的市场和语言。</p>
<p class="translated">然而，任何这种对人工智能风险的不平等反应显然意味着让一些用户比其他人面临更大的伤害风险——在已经是一个多方面的问题的多头上增加了另一个有害的层面和不公平的层面。</p>
<p class="translated">这也是为什么让强大的平台给自己的人工智能打分、给自己的作业打分、反驳自私公关的真实担忧是一件很无聊的事情。</p>
<p class="translated">(在发给我们的补充背景说明中，谷歌称自己是业内第一家将“权威性”纳入其搜索和发现算法的公司——但没有解释它声称何时做到了这一点，也没有解释在不考虑信息来源的相对价值的情况下，它如何想象自己能够实现其“组织世界信息，并使其普遍可用和有用”的既定使命。所以有色人种对这种说法感到困惑。最有可能的是，这是一个笨拙的尝试，向竞争对手抛出虚假信息。)</p>
<p class="translated">回到监管点，欧盟的一项提案——数字服务法案——将对大型数字平台提出一些透明度要求，作为更广泛的问责措施的一部分。当被问及此事时，Geurkink 形容 DSA 是“提高透明度的一个有希望的途径”</p>
<p class="translated">但她建议立法需要进一步解决像 YouTube 人工智能这样的推荐系统。</p>
<p class="translated">“我认为推荐系统的透明度特别重要，人们可以控制自己数据的输入，然后输出推荐，这是 DSA 目前有点缺乏的地方，所以我认为这是我们真正需要深入研究的地方，”她告诉我们。</p>
<p class="translated">她表示支持的一个想法是将“数据访问框架”纳入法律，以使经过审查的研究人员能够获得更多研究强大人工智能技术所需的信息，而不是像她所说的那样，法律试图提出“所有应该适用的不同透明度和信息的洗衣清单”。</p>
<p class="translated">欧盟现在也有一份人工智能法规草案摆在桌面上。立法计划采取基于风险的方法来监管人工智能的某些应用。然而，尚不清楚 YouTube 的推荐系统是否会属于监管更严格的类别之一，或者更有可能的是(至少在最初的委员会提案中)，完全不在计划中的法律范围之内。</p>
<p class="translated">“该提案的早期草案谈到了操纵人类行为的系统，这本质上就是推荐系统。从某种意义上说，这也是广告的目标。因此，很难准确理解推荐系统会陷入什么样的境地，”格尤金克指出。</p>
<p class="translated">“DSA 中一些强大的数据访问条款和新的人工智能法规之间可能会有一个很好的和谐，”她补充道。“我认为归根结底就是透明度，所以任何能够提供更大透明度的东西都是好东西。</p>
<p class="translated">“YouTube 也可以提供很多这样的东西……我们已经在这方面努力了多年，我们还没有看到他们在这方面采取任何有意义的行动，但我认为，这也是我们希望记住的事情——立法显然需要数年时间。因此，即使我们的一些建议被(谷歌)采纳，那也将是朝着正确方向迈出的一大步。”</p>




			</div>

			</div>    
</body>
</html>