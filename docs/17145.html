<html>
<head>
<title>Apple confirms it will begin scanning iCloud Photos for child abuse images </title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">苹果确认将开始扫描 iCloud 照片中的虐童图片</h1>
<blockquote>原文：<a href="https://web.archive.org/web/http://techcrunch.com/2021/08/05/apple-icloud-photos-scanning/">https://web.archive.org/web/http://techcrunch.com/2021/08/05/apple-icloud-photos-scanning/</a></blockquote><div><div class="article-content">
				<p id="speakable-summary" class="translated">今年晚些时候，苹果将推出一项技术，该技术将允许该公司检测并向执法部门报告已知的儿童性虐待材料，据称这种方式将保护用户隐私。</p>
<p class="translated">苹果告诉 TechCrunch，检测儿童性虐待材料(CSAM)是几个新功能之一，旨在更好地保护使用其服务的儿童免受在线伤害，包括过滤<a href="https://web.archive.org/web/20230331170742/https://techcrunch.com/2021/08/05/new-apple-technology-will-warn-parents-and-children-about-sexually-explicit-photos-in-messages/">阻止通过儿童的 iMessage 帐户发送和接收的潜在性暴露照片</a>。当用户试图通过 Siri 和 search 搜索 CSAM 相关词汇时，另一个功能将会介入。</p>
<p class="translated">大多数云服务——Dropbox、谷歌和微软等——已经在扫描用户文件，寻找可能违反服务条款或潜在非法的内容，比如 CSAM。但苹果一直拒绝在云端扫描用户的文件，而是让用户选择在数据到达苹果的 iCloud 服务器之前对其数据进行加密。</p>
<p class="translated">苹果表示，其新的 CSAM 检测技术——neural hash——而是在用户的设备上工作，可以识别用户是否将已知的虐待儿童图像上传到 iCloud，而无需解密图像，直到达到阈值并通过一系列检查来验证内容。</p>
<p class="translated">周三，当约翰·霍普金斯大学的密码学教授马修·格林在<a href="https://web.archive.org/web/20230331170742/https://twitter.com/matthew_d_green/status/1423071186616000513">的一系列推特</a>中透露这项新技术的存在时，苹果努力的消息泄露了。这一消息遭到了一些安全专家和隐私倡导者的抵制，但也有一些用户习惯了苹果的安全和隐私方法，而大多数其他公司都没有这种方法。</p>
<p class="translated">苹果正试图通过多层加密来平息人们的担忧，这种加密方式需要多个步骤，才能进入苹果的最终人工审查。</p>
<p class="translated">NeuralHash 将<a href="https://web.archive.org/web/20230331170742/https://techcrunch.com/2021/06/07/apple-wwdc-2021-privacy-security/">登陆 iOS 15 </a>和<a href="https://web.archive.org/web/20230331170742/https://techcrunch.com/2021/06/07/apple-unveils-macos-12-monterey/"> macOS Monterey </a>，计划在未来一两个月内发布，其工作原理是将用户 iPhone 或 Mac 上的照片转换成一个独特的字母和数字串，称为 Hash。任何时候你稍微修改一个图像，它都会改变散列值并阻止匹配。苹果公司表示，NeuralHash 试图确保相同和视觉上相似的图像——如裁剪或编辑的图像——产生相同的哈希。</p>
<p>	</p>
	
<p class="translated">在图像上传到 iCloud Photos 之前，这些哈希会在设备上与儿童虐待图像的已知哈希数据库进行匹配，该数据库由儿童保护组织提供，如全国失踪和被剥削儿童中心(NCMEC)等。NeuralHash 使用一种称为私有集合交集的加密技术来检测哈希匹配，而不会暴露图像是什么或警告用户。</p>
<p class="translated">结果被上传到苹果，但不能自己阅读。苹果公司使用另一种称为阈值秘密共享的加密原理，只有当用户在其 iCloud 照片中越过已知儿童虐待图像的阈值时，它才能解密内容。苹果公司不会说这个阈值是多少，但举例来说，如果一个秘密被分成 1000 个片段，阈值是 10 张虐童内容的图片，那么这个秘密可以从这 10 张图片中的任何一张重建。</p>
<p class="translated">此时，苹果可以解密匹配的图像，手动验证内容，禁用用户帐户，并将图像报告给 NCMEC，然后传递给执法部门。苹果公司表示，这个过程比扫描云中的文件更注重隐私，因为 NeuralHash 只搜索已知的而不是新的虐待儿童图像。苹果公司表示，出现误报的几率为万亿分之一，但在账户被错误标记的情况下，有一个上诉程序。</p>
<p class="translated">苹果<a href="https://web.archive.org/web/20230331170742/http://www.apple.com/child-safety/pdf/Apple_PSI_System_Security_Protocol_and_Analysis.pdf">在其<a href="https://web.archive.org/web/20230331170742/http://www.apple.com/child-safety/">网站</a>上公布了关于 NeuralHash 如何工作的技术细节</a>，这些技术细节经过了密码学专家的审核，并受到了儿童保护组织的称赞。</p>
<p class="translated">但是，尽管打击儿童性虐待的努力得到了广泛的支持，但仍然有一个监控的组成部分，许多人会感到不舒服，交给一个算法，<a href="https://web.archive.org/web/20230331170742/https://twitter.com/ProfWoodward/status/1423156599347154946">一些安全专家呼吁</a>在苹果向用户推出这项技术之前进行更多的公开讨论。</p>
<p class="translated">一个大问题是为什么是现在而不是更早。苹果表示，其保护隐私的 CSAM 检测直到现在才出现。但是，像苹果这样的公司也面临着来自美国政府及其盟友的巨大压力，要求削弱或后门用于保护用户数据的加密技术，以允许执法部门调查严重犯罪。</p>
<p class="translated">科技巨头拒绝为他们的系统设置后门，但进一步关闭政府访问的努力面临阻力。尽管存储在 iCloud 中的数据是加密的，即使是苹果也无法访问，但路透社去年<a href="https://web.archive.org/web/20230331170742/https://www.reuters.com/article/us-apple-fbi-icloud-exclusive-idUSKBN1ZK1CT">报道</a>称，在联邦调查局抱怨这将损害调查后，苹果放弃了对用户的完整手机备份进行加密的计划。</p>
<p class="translated">关于苹果新的 CSAM 检测工具的消息，在没有公开讨论的情况下，也引发了人们的担忧，即该技术可能被滥用，向受害者提供大量虐待儿童的图像，可能导致他们的账户被标记和关闭，但苹果淡化了这种担忧，并表示人工审查将审查可能滥用的证据。</p>
<p class="translated">苹果表示，NeuralHash 将首先在美国推出，但没有说明是否会或何时在全球推出。直到最近，像脸书这样的公司被迫<a href="https://web.archive.org/web/20230331170742/https://www.bbc.com/news/technology-55399509">关闭</a>他们在整个欧盟的儿童虐待检测工具，因为这种做法被无意中禁止了。苹果表示，这项功能在技术上是可选的，因为你不必使用 iCloud Photos，但如果用户使用，这将是一项要求。毕竟，你的设备属于你，但苹果的云不属于你。</p>

			</div>

			</div>    
</body>
</html>